{
 "cells": [
  {
<<<<<<< HEAD
=======
   "cell_type": "raw",
   "id": "0bdb3b97-4989-4237-b43b-5943dbbd8302",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_position: 1.5\n",
    "---"
   ]
  },
  {
>>>>>>> langchan/master
   "cell_type": "markdown",
   "id": "bb7d49db-04d3-4399-bfe1-09f82bbe6015",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "# ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð¿Ð¾Ñ‚Ð¾ÐºÐ¾Ð²Ð¾Ð¹ Ð¿ÐµÑ€ÐµÐ´Ð°Ñ‡Ð¸ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð²\n",
    "\n",
    "ÐŸÐ¾Ñ‚Ð¾ÐºÐ¾Ð²Ð°Ñ Ð¿ÐµÑ€ÐµÐ´Ð°Ñ‡Ð° Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð² Ð½ÑƒÐ¶Ð½Ð° Ð´Ð»Ñ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ð¾Ñ‚Ð·Ñ‹Ð²Ñ‡Ð¸Ð²Ñ‹Ñ… Ð¸Ð½Ñ‚ÐµÑ€Ñ„ÐµÐ¹ÑÐ¾Ð² Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ð¹, Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÑŽÑ‰Ð¸Ñ… Ñ LLM.\n",
    "\n",
    "Ð’ Ð¾ÑÐ½Ð¾Ð²Ðµ Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ñ… Ð¿Ñ€Ð¸Ð¼Ð¸Ñ‚Ð¸Ð²Ð¾Ð² Ð±Ð¸Ð±Ð»Ð¸Ð¾Ñ‚ÐµÐºÐ¸ GigaChain Ð»ÐµÐ¶Ð¸Ñ‚ [Runnable-Ð¸Ð½Ñ‚ÐµÑ€Ñ„ÐµÐ¹Ñ](/docs/expression_language/interface), ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÐµÑ‚ Ð´Ð²Ð° Ð¾ÑÐ½Ð¾Ð²Ð½Ñ‹Ñ… Ð¿Ð¾Ð´Ñ…Ð¾Ð´Ð° Ðº Ñ€Ð°Ð±Ð¾Ñ‚Ðµ Ñ Ð¿Ð¾Ñ‚Ð¾ÐºÐ¾Ð²Ð¾Ð¹ Ð¿ÐµÑ€ÐµÐ´Ð°Ñ‡ÐµÐ¹ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð²:\n",
    "\n",
    "* ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð½Ð¾Ð³Ð¾ Ð¸ Ð°ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð½Ð¾Ð³Ð¾ Ð¼ÐµÑ‚Ð¾Ð´Ð¾Ð² `stream` Ð¸ `astream`. ÐœÐµÑ‚Ð¾Ð´Ñ‹ Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÑŽÑ‚ Ð¾Ð±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°Ñ‚ÑŒ Ð¸Ñ‚Ð¾Ð³Ð¾Ð²Ñ‹Ð¹ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ñ†ÐµÐ¿Ð¾Ñ‡ÐºÐ¸ Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑŽÑ‚ÑÑ Ð² ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ðµ Ð¾ÑÐ½Ð¾Ð²Ð½Ð¾Ð³Ð¾ Ð¼ÐµÑ‚Ð¾Ð´Ð° Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ñ Ð¿Ð¾Ñ‚Ð¾ÐºÐ¾Ð²Ð¾Ð¹ Ð¿ÐµÑ€ÐµÐ´Ð°Ñ‡ÐµÐ¹.\n",
    "* ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Ð°ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð½Ñ‹Ñ… Ð¼ÐµÑ‚Ð¾Ð´Ð¾Ð² `astream_events` Ð¸ `astream_log`. Ð¡ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ ÑÑ‚Ð¸Ñ… Ð¼ÐµÑ‚Ð¾Ð´Ð¾Ð² Ð¼Ð¾Ð¶Ð½Ð¾ Ð¾Ð±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°Ñ‚ÑŒ Ð¿Ñ€Ð¾Ð¼ÐµÐ¶ÑƒÑ‚Ð¾Ñ‡Ð½Ñ‹Ðµ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ñ†ÐµÐ¿Ð¾Ñ‡ÐºÐ¸ Ð½Ð°Ñ€ÑÐ´Ñƒ Ñ Ð¸Ñ‚Ð¾Ð³Ð¾Ð²Ñ‹Ð¼Ð¸.\n",
    "\n",
    "## Ð Ð°Ð±Ð¾Ñ‚Ð° Ñ Ð¼ÐµÑ‚Ð¾Ð´Ð°Ð¼Ð¸ stream Ð¸ astream\n",
    "\n",
    "ÐœÐµÑ‚Ð¾Ð´Ñ‹ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ð¿Ð¾Ñ‚Ð¾ÐºÐ¾Ð²Ð¾Ð¹ Ð¿ÐµÑ€ÐµÐ´Ð°Ñ‡Ð¸ `stream` (ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð½Ñ‹Ð¹) Ð¸ `astream` (Ð°ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð½Ñ‹Ð¹) Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹ Ð´Ð»Ñ Ð²ÑÐµÑ… Runnable-Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð².\n",
    "ÐœÐµÑ‚Ð¾Ð´Ñ‹ Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÑŽÑ‚ Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚Ñ‹ Ð¸Ñ‚Ð¾Ð³Ð¾Ð²Ð¾Ð³Ð¾ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð° Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ñ†ÐµÐ¿Ð¾Ñ‡ÐºÐ¸, ÐºÐ°Ðº Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¾Ð½Ð¸ ÑÑ‚Ð°Ð½Ð¾Ð²ÑÑ‚ÑÑ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹.\n",
    "\n",
    "ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð¿Ð¾Ñ‚Ð¾ÐºÐ¾Ð²Ð¾Ð¹ Ð¿ÐµÑ€ÐµÐ´Ð°Ñ‡Ð¸ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð² Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð° Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÐµÑÐ»Ð¸ ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ñ‹ Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ñ‹ Ð¼Ð¾Ð³ÑƒÑ‚ Ð¾Ð±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°Ñ‚ÑŒ Ð¿Ð¾Ñ‚Ð¾Ðº Ð²Ñ…Ð¾Ð´Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ….\n",
    "Ð¢Ð¾ ÐµÑÑ‚ÑŒ, Ð¾Ð±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°Ñ‚ÑŒ Ð²Ñ…Ð¾Ð´Ð½Ñ‹Ðµ Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚Ñ‹ Ð¾Ð´Ð¸Ð½ Ð·Ð° Ð´Ñ€ÑƒÐ³Ð¸Ð¼ Ð¸ Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°Ñ‚ÑŒ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ð¸Ð· Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚Ð¾Ð².\n",
    "\n",
    "Ð¡Ð»Ð¾Ð¶Ð½Ð¾ÑÑ‚ÑŒ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ð¿Ð¾Ñ‚Ð¾ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð·Ð°Ð²Ð¸ÑÐ¸Ñ‚ Ð¾Ñ‚ Ð·Ð°Ð´Ð°Ñ‡, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð²Ð°Ð¼ Ð½ÑƒÐ¶Ð½Ð¾ Ñ€ÐµÑˆÐ¸Ñ‚ÑŒ.\n",
    "Ð­Ñ‚Ð¾ Ð¼Ð¾Ð¶ÐµÑ‚ Ð±Ñ‹Ñ‚ÑŒ ÐºÐ°Ðº Ð¾Ð±Ñ‹Ñ‡Ð½Ð°Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð², ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÑ‚ Ð¼Ð¾Ð´ÐµÐ»ÑŒ, Ñ‚Ð°Ðº Ð¸ Ð±Ð¾Ð»ÐµÐµ ÑÐ»Ð¾Ð¶Ð½Ñ‹Ðµ Ð·Ð°Ð´Ð°Ñ‡Ð¸ Ð¿Ð¾ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐµ Ñ‡Ð°ÑÑ‚ÐµÐ¹ JSON-Ñ„Ð°Ð¹Ð»Ð° Ð´Ð¾ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¸Ñ‚Ð¾Ð³Ð¾Ð²Ð¾Ð³Ð¾ Ñ„Ð°Ð¹Ð»Ð°.\n",
    "\n",
    "<!--\n",
=======
    "# How to stream runnables\n",
    "\n",
    ":::info Prerequisites\n",
    "\n",
    "This guide assumes familiarity with the following concepts:\n",
    "- [Chat models](/docs/concepts/#chat-models)\n",
    "- [LangChain Expression Language](/docs/concepts/#langchain-expression-language)\n",
    "- [Output parsers](/docs/concepts/#output-parsers)\n",
    "\n",
    ":::\n",
    "\n",
    "Streaming is critical in making applications based on LLMs feel responsive to end-users.\n",
    "\n",
    "Important LangChain primitives like [chat models](/docs/concepts/#chat-models), [output parsers](/docs/concepts/#output-parsers), [prompts](/docs/concepts/#prompt-templates), [retrievers](/docs/concepts/#retrievers), and [agents](/docs/concepts/#agents) implement the LangChain [Runnable Interface](/docs/concepts#interface).\n",
    "\n",
    "This interface provides two general approaches to stream content:\n",
    "\n",
    "1. sync `stream` and async `astream`: a **default implementation** of streaming that streams the **final output** from the chain.\n",
    "2. async `astream_events` and async `astream_log`: these provide a way to stream both **intermediate steps** and **final output** from the chain.\n",
    "\n",
    "Let's take a look at both approaches, and try to understand how to use them.\n",
    "\n",
    "## Using Stream\n",
    "\n",
    "All `Runnable` objects implement a sync method called `stream` and an async variant called `astream`. \n",
    "\n",
    "These methods are designed to stream the final output in chunks, yielding each chunk as soon as it is available.\n",
    "\n",
    "Streaming is only possible if all steps in the program know how to process an **input stream**; i.e., process an input chunk one at a time, and yield a corresponding output chunk.\n",
    "\n",
    "The complexity of this processing can vary, from straightforward tasks like emitting tokens produced by an LLM, to more challenging ones like streaming parts of JSON results before the entire JSON is complete.\n",
    "\n",
    "The best place to start exploring streaming is with the single most important components in LLMs apps-- the LLMs themselves!\n",
    "\n",
>>>>>>> langchan/master
    "### LLMs and Chat Models\n",
    "\n",
    "Large language models and their chat variants are the primary bottleneck in LLM based apps.\n",
    "\n",
    "Large language models can take **several seconds** to generate a complete response to a query. This is far slower than the **~200-300 ms** threshold at which an application feels responsive to an end user.\n",
    "\n",
    "The key strategy to make the application feel more responsive is to show intermediate progress; viz., to stream the output from the model **token by token**.\n",
<<<<<<< HEAD
    "-->"
=======
    "\n",
    "We will show examples of streaming using a chat model. Choose one from the options below:\n",
    "\n",
    "```{=mdx}\n",
    "import ChatModelTabs from \"@theme/ChatModelTabs\";\n",
    "\n",
    "<ChatModelTabs\n",
    "  customVarName=\"model\"\n",
    "/>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f123bdcb-8c8b-440c-9bbd-aa5ed4e9cd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "%pip install -qU langchain langchain_anthropic\n",
    "\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "keys = [\n",
    "    \"ANTHROPIC_API_KEY\",\n",
    "    \"OPENAI_API_KEY\",\n",
    "]\n",
    "\n",
    "for key in keys:\n",
    "    if key not in os.environ:\n",
    "        os.environ[key] = getpass(f\"Enter API Key for {key}=?\")\n",
    "\n",
    "\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "model = ChatAnthropic(model=\"claude-3-sonnet-20240229\", temperature=0)"
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2464c57-0e89-4159-b21f-5859a21be658",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "Ð”Ð»Ñ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð°Ñ†Ð¸Ð¸ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ñ Ð¿Ð¾Ñ‚Ð¾ÐºÐ¾Ð²Ð¾Ð¹ Ð¿ÐµÑ€ÐµÐ´Ð°Ñ‡ÐµÐ¹ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð² Ð¸ÑÑ‚Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÑŒ GigaChat, Ð¸Ð½Ñ‚ÐµÐ³Ñ€Ð°Ñ†Ð¸Ñ Ñ ÐºÐ¾Ñ‚Ð¾Ñ€Ð¾Ð¹ Ð²Ñ…Ð¾Ð´Ð¸Ñ‚ Ð² Ð¾ÑÐ½Ð¾Ð²Ð½ÑƒÑŽ Ð±Ð¸Ð»Ð¸Ð¾Ñ‚ÐµÐºÑƒ GigaChain."
=======
    "Let's start with the sync `stream` API:"
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b44dfb2-0749-487a-8918-f8b6b8233093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The| sky| appears| blue| during| the| day|.|"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "pip install -qU gigachain"
=======
    "chunks = []\n",
    "for chunk in model.stream(\"what color is the sky?\"):\n",
    "    chunks.append(chunk)\n",
    "    print(chunk.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d835b5c-cbb7-41ab-8905-bdc24d515d29",
   "metadata": {},
   "source": [
    "Alternatively, if you're working in an async environment, you may consider using the async `astream` API:"
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
<<<<<<< HEAD
   "id": "91787fc7-d941-48c0-a8b4-0ee61ab7dd5d",
=======
   "id": "f180b6a0-0027-4bd8-8bab-fde76e282609",
>>>>>>> langchan/master
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "ÐŸÑ€Ð¸Ð²ÐµÑ‚! Ð¯ - GigaChat, Ð²Ð¸Ñ€Ñ‚ÑƒÐ°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¿Ð¾Ð¼Ð¾Ñ‰Ð½Ð¸Ðº, ÑÐ¾Ð·Ð´Ð°Ð½Ð½Ñ‹Ð¹ ÐºÐ¾Ð¼Ð¿Ð°Ð½Ð¸ÐµÐ¹ Ð¡Ð±ÐµÑ€. Ð¯ Ð¼Ð¾Ð³Ñƒ Ð¾Ñ‚Ð²ÐµÑ‡Ð°Ñ‚ÑŒ Ð½Ð° Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹, Ð´Ð°Ð²Ð°Ñ‚ÑŒ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¸ Ð¿Ð¾Ð¼Ð¾Ð³Ð°Ñ‚ÑŒ Ð² Ñ€ÐµÑˆÐµÐ½Ð¸Ð¸ Ð·Ð°Ð´Ð°Ñ‡. Ð¯ Ð¾Ð±ÑƒÑ‡ÐµÐ½ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… Ð¾Ð±ÑŠÐµÐ¼Ð¾Ð² Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸ Ð¸ÑÐºÑƒÑÑÑ‚Ð²ÐµÐ½Ð½Ð¾Ð³Ð¾ Ð¸Ð½Ñ‚ÐµÐ»Ð»ÐµÐºÑ‚Ð°, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð»ÑÑ‚ÑŒ Ñ‚Ð¾Ñ‡Ð½Ñ‹Ðµ Ð¸ Ð¿Ð¾Ð»ÐµÐ·Ð½Ñ‹Ðµ Ð¾Ñ‚Ð²ÐµÑ‚Ñ‹. Ð•ÑÐ»Ð¸ Ñƒ Ð²Ð°Ñ ÐµÑÑ‚ÑŒ ÐºÐ°ÐºÐ¸Ðµ-Ð»Ð¸Ð±Ð¾| Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹| Ð¸Ð»Ð¸| Ð½ÑƒÐ¶Ð½Ð°| Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒ|,| Ñ| Ð±ÑƒÐ´Ñƒ| Ñ€Ð°|Ð´| Ð¿Ð¾Ð¼Ð¾Ñ‡ÑŒ| Ð²Ð°Ð¼|.||"
=======
      "The| sky| appears| blue| during| the| day|.|"
>>>>>>> langchan/master
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "from langchain.chat_models.gigachat import GigaChat\n",
    "\n",
    "model = GigaChat(\n",
    "    credentials=\"NjgxZmQ1Y2ItMWVmNS00Y2ZmLWE2MDgtZmZjNDM0NDA3NmJlOjA2NTNhNjAwLTU5NmYtNGQ0Ni05N2MxLTVhYjI0NWU0OTBlOA==\",\n",
    "    verify_ssl_certs=False,\n",
    "    scope=\"GIGACHAT_API_CORP\",\n",
    ")\n",
    "\n",
    "chunks = []\n",
    "async for chunk in model.astream(\"ÐŸÑ€Ð¸Ð²ÐµÑ‚. Ð Ð°ÑÑÐºÐ°Ð¶Ð¸ Ð¿Ð¾Ð±Ð¾Ð»ÑŒÑˆÐµ Ð¾ ÑÐµÐ±Ðµ\"):\n",
=======
    "chunks = []\n",
    "async for chunk in model.astream(\"what color is the sky?\"):\n",
>>>>>>> langchan/master
    "    chunks.append(chunk)\n",
    "    print(chunk.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66730a87-77d5-40d6-a68f-315121989bd1",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    ":::note\n",
    "\n",
    "ÐŸÐµÑ€Ð²Ñ‹Ð¹ Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚ Ð¾Ñ‚Ð²ÐµÑ‚Ð° GigaChat Ð² Ð¿Ð¾Ñ‚Ð¾ÐºÐ¾Ð¼ Ñ€ÐµÐ¶Ð¸Ð¼Ðµ Ð²ÑÐµÐ³Ð´Ð° ÑÐ°Ð¼Ñ‹Ð¹ Ð±Ð¾Ð»ÑŒÑˆÐ¾Ð¹.\n",
    "\n",
    ":::\n",
    "\n",
    "ÐŸÐ¾ÑÐ¼Ð¾Ñ‚Ñ€Ð¸Ð¼ Ð½Ð° Ð¾Ð´Ð¸Ð½ Ð¸Ð· Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚Ð¾Ð²."
=======
    "Let's inspect one of the chunks"
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dade3000-1ac4-4f5c-b5c6-a0217f9f8a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "AIMessageChunk(content=' Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹', id='run-0f017856-0fdb-4a42-b766-b87cab567eed')"
=======
       "AIMessageChunk(content='The', id='run-b36bea64-5511-4d7a-b6a3-a07b3db0c8e7')"
>>>>>>> langchan/master
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "chunks[1]"
=======
    "chunks[0]"
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a47193-2bd1-46bc-9c7e-ea0f6b08c4a5",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "Ð¤Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚ ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ñ‚ `AIMessageChunk` â€” Ñ‡Ð°ÑÑ‚ÑŒ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ `AIMessage`.\n",
    "\n",
    "Ð¤Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚Ñ‹ ÑÐ¾Ð·Ð´Ð°Ð½Ñ‹ Ñ‚Ð°ÐºÐ¸Ð¼ Ð¾Ð±Ñ€Ð°Ð·Ð¾Ð¼, Ñ‡Ñ‚Ð¾ Ð¸Ñ… Ð¼Ð¾Ð¶Ð½Ð¾ Ð´Ð¾Ð±Ð°Ð²Ð»ÑÑ‚ÑŒ Ð´Ñ€ÑƒÐ³ Ðº Ð´Ñ€ÑƒÐ³Ñƒ, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð²Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ð¸ÑÑ…Ð¾Ð´Ð½Ð¾Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ."
=======
    "We got back something called an `AIMessageChunk`. This chunk represents a part of an `AIMessage`.\n",
    "\n",
    "Message chunks are additive by design -- one can simply add them up to get the state of the response so far!"
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3cf5f38-249c-4da0-94e6-5e5203fad52e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "AIMessageChunk(content='ÐŸÑ€Ð¸Ð²ÐµÑ‚! Ð¯ - GigaChat, Ð²Ð¸Ñ€Ñ‚ÑƒÐ°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¿Ð¾Ð¼Ð¾Ñ‰Ð½Ð¸Ðº, ÑÐ¾Ð·Ð´Ð°Ð½Ð½Ñ‹Ð¹ ÐºÐ¾Ð¼Ð¿Ð°Ð½Ð¸ÐµÐ¹ Ð¡Ð±ÐµÑ€. Ð¯ Ð¼Ð¾Ð³Ñƒ Ð¾Ñ‚Ð²ÐµÑ‡Ð°Ñ‚ÑŒ Ð½Ð° Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹, Ð´Ð°Ð²Ð°Ñ‚ÑŒ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¸ Ð¿Ð¾Ð¼Ð¾Ð³Ð°Ñ‚ÑŒ Ð² Ñ€ÐµÑˆÐµÐ½Ð¸Ð¸ Ð·Ð°Ð´Ð°Ñ‡. Ð¯ Ð¾Ð±ÑƒÑ‡ÐµÐ½ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… Ð¾Ð±ÑŠÐµÐ¼Ð¾Ð² Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸ Ð¸ÑÐºÑƒÑÑÑ‚Ð²ÐµÐ½Ð½Ð¾Ð³Ð¾ Ð¸Ð½Ñ‚ÐµÐ»Ð»ÐµÐºÑ‚Ð°, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð»ÑÑ‚ÑŒ Ñ‚Ð¾Ñ‡Ð½Ñ‹Ðµ Ð¸ Ð¿Ð¾Ð»ÐµÐ·Ð½Ñ‹Ðµ Ð¾Ñ‚Ð²ÐµÑ‚Ñ‹. Ð•ÑÐ»Ð¸ Ñƒ Ð²Ð°Ñ ÐµÑÑ‚ÑŒ ÐºÐ°ÐºÐ¸Ðµ-Ð»Ð¸Ð±Ð¾ Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹ Ð¸Ð»Ð¸ Ð½ÑƒÐ¶Ð½Ð° Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒ', id='run-0f017856-0fdb-4a42-b766-b87cab567eed')"
=======
       "AIMessageChunk(content='The sky appears blue during', id='run-b36bea64-5511-4d7a-b6a3-a07b3db0c8e7')"
>>>>>>> langchan/master
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0] + chunks[1] + chunks[2] + chunks[3] + chunks[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ffbd9a-3b79-44b6-8883-1371f9460c77",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "### Ð Ð°Ð±Ð¾Ñ‚Ð° Ñ Ñ†ÐµÐ¿Ð¾Ñ‡ÐºÐ°Ð¼Ð¸\n",
    "\n",
    "ÐšÐ°Ðº Ð¿Ñ€Ð°Ð²Ð¸Ð»Ð¾, Ð·Ð°Ð¿Ñ€Ð¾Ñ Ðº Ð¼Ð¾Ð´ÐµÐ»Ð¸ ÑÑ‚Ð¾ Ð»Ð¸ÑˆÑŒ Ð¾Ð´Ð½Ð° Ð¸Ð· Ñ‡Ð°ÑÑ‚ÐµÐ¹ Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ.\n",
    "\n",
    "ÐÐ¸Ð¶Ðµ Ñ€Ð°Ð±Ð¾Ñ‚Ð° Ð¿Ð¾Ñ‚Ð¾ÐºÐ¾Ð²Ð¾Ð¹ Ð¿ÐµÑ€ÐµÐ´Ð°Ñ‡Ð¸ Ð¿Ð¾ÐºÐ°Ð·Ð°Ð½Ð° Ð½Ð° Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ðµ Ñ†ÐµÐ¿Ð¾Ñ‡ÐºÐ¸ ÑÐ¾Ð·Ð´Ð°Ð½Ð½Ð¾Ð¹ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ LangChain Expression Language (LCEL).\n",
    "Ð¦ÐµÐ¿Ð¾Ñ‡ÐºÐ° ÑÐ¾ÑÑ‚Ð¾Ð¸Ñ‚ Ð¸Ð· Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð°, Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¸ Ð¿Ð°Ñ€ÑÐµÑ€Ð°.\n",
    "\n",
    "Ð”Ð»Ñ Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð° Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½ Ð¿Ð°Ñ€ÑÐµÑ€ `StrOutputParser`, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð¸Ð·Ð²Ð»ÐµÐºÐ°ÐµÑ‚ Ð¿Ð¾Ð»Ðµ `content` Ð¸Ð· ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ `AIMessageChunk` Ð¸ Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ñ‚Ð¾ÐºÐµÐ½, Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð½Ñ‹Ð¹ Ð¾Ñ‚ Ð¼Ð¾Ð´ÐµÐ»Ð¸.\n",
    "\n",
    ":::note\n",
    "\n",
    "LCEL â€” Ð´ÐµÐºÐ»Ð°Ñ€Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ð¹ ÑÐ·Ñ‹Ðº Ð´Ð»Ñ ÑÐ¾ÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ñ Ð¿Ñ€Ð¸Ð¼Ð¸Ñ‚Ð¸Ð²Ð¾Ð² GigaChain Ð² Ñ†ÐµÐ¿Ð¾Ñ‡ÐºÐ¸.\n",
    "ÐœÐµÑ‚Ð¾Ð´Ñ‹ `stream` Ð¸ `astream` Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹ Ð² LCEL-Ñ†ÐµÐ¿Ð¾Ñ‡ÐºÐ°Ñ…, Ñ‡Ñ‚Ð¾ Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÑ‚ Ð¾Ð±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°Ñ‚ÑŒ Ð¸Ñ‚Ð¾Ð³Ð¾Ð²Ñ‹Ð¹ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¼Ð¾Ð´ÐµÐ»Ð¸.\n",
    "\n",
    "ÐšÑ€Ð¾Ð¼Ðµ ÑÑ‚Ð¸Ñ… Ð¼ÐµÑ‚Ð¾Ð´Ð¾Ð² Ñ†ÐµÐ¿Ð¾Ñ‡ÐºÐ¸ Ñ€ÐµÐ°Ð»Ð¸Ð·ÑƒÑŽÑ‚ Ð²ÐµÑÑŒ ÑÑ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð½Ñ‹Ð¹ Runnable-Ð¸Ð½Ñ‚ÐµÑ€Ñ„ÐµÐ¹Ñ.\n",
    "\n",
=======
    "### Chains\n",
    "\n",
    "Virtually all LLM applications involve more steps than just a call to a language model.\n",
    "\n",
    "Let's build a simple chain using `LangChain Expression Language` (`LCEL`) that combines a prompt, model and a parser and verify that streaming works.\n",
    "\n",
    "We will use [`StrOutputParser`](https://api.python.langchain.com/en/latest/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html) to parse the output from the model. This is a simple parser that extracts the `content` field from an `AIMessageChunk`, giving us the `token` returned by the model.\n",
    "\n",
    ":::{.callout-tip}\n",
    "LCEL is a *declarative* way to specify a \"program\" by chainining together different LangChain primitives. Chains created using LCEL benefit from an automatic implementation of `stream` and `astream` allowing streaming of the final output. In fact, chains created with LCEL implement the entire standard Runnable interface.\n",
>>>>>>> langchan/master
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8562ae2-3fd1-4829-9801-a5a732b1798d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here|'s| a| joke| about| a| par|rot|:|\n",
      "\n",
      "A man| goes| to| a| pet| shop| to| buy| a| par|rot|.| The| shop| owner| shows| him| two| stunning| pa|rr|ots| with| beautiful| pl|um|age|.|\n",
      "\n",
      "\"|There|'s| a| talking| par|rot| an|d a| non|-|talking| par|rot|,\"| the| owner| says|.| \"|The| talking| par|rot| costs| $|100|,| an|d the| non|-|talking| par|rot| is| $|20|.\"|\n",
      "\n",
      "The| man| says|,| \"|I|'ll| take| the| non|-|talking| par|rot| at| $|20|.\"|\n",
      "\n",
      "He| pays| an|d leaves| with| the| par|rot|.| As| he|'s| walking| down| the| street|,| the| par|rot| looks| up| at| him| an|d says|,| \"|You| know|,| you| really| are| a| stupi|d man|!\"|\n",
      "\n",
      "The| man| is| stun|ne|d an|d looks| at| the| par|rot| in| dis|bel|ief|.| The| par|rot| continues|,| \"|Yes|,| you| got| r|ippe|d off| big| time|!| I| can| talk| just| as| well| as| that| other| par|rot|,| an|d you| only| pai|d $|20| |for| me|!\"|"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
<<<<<<< HEAD
    "prompt = ChatPromptTemplate.from_template(\"Ñ€Ð°ÑÑÐºÐ°Ð¶Ð¸ ÑˆÑƒÑ‚ÐºÑƒ Ð¾ {topic}\")\n",
    "parser = StrOutputParser()\n",
    "chain = prompt | model | parser\n",
    "\n",
    "async for chunk in chain.astream({\"topic\": \"Ð¿Ð¾Ð¿ÑƒÐ³Ð°Ð¹\"}):\n",
=======
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "parser = StrOutputParser()\n",
    "chain = prompt | model | parser\n",
    "\n",
    "async for chunk in chain.astream({\"topic\": \"parrot\"}):\n",
>>>>>>> langchan/master
    "    print(chunk, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868bc412",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "ÐŸÐ°Ñ€ÑÐµÑ€ `parser` Ð¾Ð±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÑ‚ Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚Ñ‹ Ð¾Ð´Ð¸Ð½ Ð·Ð° Ð´Ñ€ÑƒÐ³Ð¸Ð¼ Ð¿Ð¾ Ð¼ÐµÑ€Ðµ Ð¸Ñ… Ð¿Ð¾ÑÑ‚ÑƒÐ¿Ð»ÐµÐ½Ð¸Ñ Ð¾Ñ‚ Ð¼Ð¾Ð´ÐµÐ»Ð¸.\n",
    "Ð¢Ð°ÐºÐ¾Ðµ Ð¿Ð¾Ð²ÐµÑ€ÐµÐ´Ð¸Ðµ Ð¿Ñ€Ð¸ Ñ€Ð°Ð±Ð¾Ñ‚Ðµ Ñ Ð¿Ð¾Ñ‚Ð¾ÐºÐ¾Ð²Ð¾Ð¹ Ð¿ÐµÑ€ÐµÐ´Ð°Ñ‡ÐµÐ¹ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ñ…Ð°Ñ€Ð°ÐºÑ‚ÐµÑ€Ð½Ð¾ Ð´Ð»Ñ Ð¼Ð½Ð¾Ð³Ð¸Ñ… Ð¿Ñ€Ð¸Ð¼Ð¸Ñ‚Ð¸Ð²Ð¾Ð² LCEL.\n",
    "\n",
    "Ð¢ÐµÐ¼ Ð½Ðµ Ð¼ÐµÐ½ÐµÐµ, Ð½ÐµÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Runnable, Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€ [ÑˆÐ°Ð±Ð»Ð¾Ð½Ñ‹ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð¾Ð²](/docs/modules/model_io/prompts) Ð¸Ð»Ð¸ [Ñ‡Ð°Ñ‚-Ð¼Ð¾Ð´ÐµÐ»Ð¸](/docs/modules/model_io/chat), Ð½Ðµ ÑƒÐ¼ÐµÑŽÑ‚ Ð¾Ð±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°Ñ‚ÑŒ Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚Ñ‹ Ð¸, Ð²Ð¼ÐµÑÑ‚Ð¾ ÑÑ‚Ð¾Ð³Ð¾, Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½ÑÑŽÑ‚ Ð²ÑÐµ Ð¿Ñ€ÐµÐ´Ñ‹Ð´ÑƒÑ‰Ð¸Ðµ ÑÑ‚Ð°Ð¿Ñ‹ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ñ†ÐµÐ¿Ð¾Ñ‡ÐºÐ¸.\n",
    "Ð¢Ð°ÐºÐ¾Ðµ Ð¿Ð¾Ð²ÐµÐ´ÐµÐ½Ð¸Ðµ Ð²ÐµÐ´ÐµÑ‚ Ðº Ð¿Ñ€ÐµÑ€Ñ‹Ð²Ð°Ð½Ð¸ÑŽ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ð¿Ð¾Ñ‚Ð¾ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ…."
=======
    "Note that we're getting streaming output even though we're using `parser` at the end of the chain above. The `parser` operates on each streaming chunk individidually. Many of the [LCEL primitives](/docs/how_to#langchain-expression-language-lcel) also support this kind of transform-style passthrough streaming, which can be very convenient when constructing apps. \n",
    "\n",
    "Custom functions can be [designed to return generators](/docs/how_to/functions#streaming), which are able to operate on streams.\n",
    "\n",
    "Certain runnables, like [prompt templates](/docs/how_to#prompt-templates) and [chat models](/docs/how_to#chat-models), cannot process individual chunks and instead aggregate all previous steps. Such runnables can interrupt the streaming process."
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b399fb4-5e3c-4581-9570-6df9b42b623d",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "<!--\n",
    "\n",
=======
>>>>>>> langchan/master
    ":::{.callout-note}\n",
    "The LangChain Expression language allows you to separate the construction of a chain from the mode in which it is used (e.g., sync/async, batch/streaming etc.). If this is not relevant to what you're building, you can also rely on a standard **imperative** programming approach by\n",
    "caling `invoke`, `batch` or `stream` on each component individually, assigning the results to variables and then using them downstream as you see fit.\n",
    "\n",
<<<<<<< HEAD
    "If that works for your needs, then that's fine by us ðŸ‘Œ!\n",
    ":::\n",
    "-->"
=======
    ":::"
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfff2701-8887-486f-8b3b-eb26383d4bb6",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "### ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð¿Ð¾Ñ‚Ð¾ÐºÐ° Ð²Ñ…Ð¾Ð´Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…\n",
    "\n",
    "ÐŸÑ€ÐµÐ´Ð¿Ð¾Ð»Ð¾Ð¶Ð¸Ð¼, Ð²Ð°Ð¼ Ð½ÑƒÐ¶Ð½Ð¾ Ñ‚Ñ€Ð°Ð½ÑÐ»Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ JSON Ð¿Ð¾ Ñ…Ð¾Ð´Ñƒ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸.\n",
    "\n",
    "Ð•ÑÐ»Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ð´Ð»Ñ Ñ€Ð°Ð·Ð±Ð¾Ñ€Ð° Ñ‡Ð°ÑÑ‚Ð¸Ñ‡Ð½Ð¾Ð³Ð¾ JSON-Ð¾Ð±ÑŠÐµÐºÑ‚Ð° Ð¼ÐµÑ‚Ð¾Ð´ `json.loads`, Ð±ÑƒÐ´ÐµÑ‚ Ð²Ð¾Ð·Ð½Ð¸ÐºÐ°Ñ‚ÑŒ Ð¾ÑˆÐ¸Ð±ÐºÐ°, Ð¿Ð¾Ñ‚Ð¾Ð¼Ñƒ Ñ‡Ñ‚Ð¾ Ñ‡Ð°ÑÑ‚Ð¸Ñ‡Ð½Ñ‹Ð¹ JSON Ð½Ðµ ÑÐ²Ð»ÑÐµÑ‚ÑÑ Ð²Ð°Ð»Ð¸Ð´Ð½Ñ‹Ð¼.\n",
    "\n",
    "Ð”Ð»Ñ Ñ€ÐµÑˆÐµÐ½Ð¸Ñ ÑÑ‚Ð¾Ð¹ Ð·Ð°Ð´Ð°Ñ‡Ð¸ Ð¿Ð°Ñ€ÑÐµÑ€ Ð´Ð¾Ð»Ð¶ÐµÐ½ Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ Ñ Ð²Ñ…Ð¾Ð´Ð½Ñ‹Ð¼ Ð¿Ð¾Ñ‚Ð¾ÐºÐ¾Ð¼ Ð¸ Ð¿Ñ‹Ñ‚Ð°Ñ‚ÑŒÑÑ Â«Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸ Ð·Ð°Ð²ÐµÑ€ÑˆÐ°Ñ‚ÑŒÂ» Ñ‡Ð°ÑÑ‚Ð¸Ñ‡Ð½Ñ‹Ð¹ JSON Ð´Ð¾ Ð¿Ñ€Ð¸Ð³Ð½Ð¾Ð´Ð½Ð¾Ð³Ð¾ Ðº Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÑŽ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ñ."
=======
    "### Working with Input Streams\n",
    "\n",
    "What if you wanted to stream JSON from the output as it was being generated?\n",
    "\n",
    "If you were to rely on `json.loads` to parse the partial json, the parsing would fail as the partial json wouldn't be valid json.\n",
    "\n",
    "You'd likely be at a complete loss of what to do and claim that it wasn't possible to stream JSON.\n",
    "\n",
    "Well, turns out there is a way to do it -- the parser needs to operate on the **input stream**, and attempt to \"auto-complete\" the partial json into a valid state.\n",
    "\n",
    "Let's see such a parser in action to understand what this means."
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ff63cce-715a-4561-951f-9321c82e8d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "{'countries': []}\n",
      "{'countries': [{}]}\n",
      "{'countries': [{'name': ''}]}\n",
      "{'countries': [{'name': 'France'}]}\n",
      "{'countries': [{'name': 'France', 'population': 67}]}\n",
      "{'countries': [{'name': 'France', 'population': 67413}]}\n",
      "{'countries': [{'name': 'France', 'population': 67413000}]}\n",
      "{'countries': [{'name': 'France', 'population': 67413000}, {}]}\n",
      "{'countries': [{'name': 'France', 'population': 67413000}, {'name': ''}]}\n",
      "{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain'}]}\n",
      "{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47}]}\n",
      "{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47351}]}\n",
      "{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47351567}]}\n",
      "{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47351567}, {}]}\n",
      "{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47351567}, {'name': ''}]}\n",
      "{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47351567}, {'name': 'Japan'}]}\n",
      "{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47351567}, {'name': 'Japan', 'population': 125}]}\n",
      "{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47351567}, {'name': 'Japan', 'population': 125584}]}\n",
      "{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47351567}, {'name': 'Japan', 'population': 125584000}]}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "chain = (\n",
    "    model | JsonOutputParser()\n",
<<<<<<< HEAD
    ")  # Ð‘Ð°Ð³ Ð² Ñ€Ð°Ð½Ð½Ð¸Ñ… Ð²ÐµÑ€ÑÐ¸ÑÑ… GigaChain Ð¿Ñ€Ð¸Ð²Ð¾Ð´Ð¸Ð» Ðº Ñ‚Ð¾Ð¼Ñƒ, Ñ‡Ñ‚Ð¾ JsonOutputParser Ð½Ðµ Ñ‚Ñ€Ð°Ð½ÑÐ»Ð¸Ñ€Ð¾Ð²Ð°Ð» Ñ‚Ð¾ÐºÐµÐ½Ñ‹, Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð½Ñ‹Ðµ Ð¾Ñ‚ Ð½ÐµÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹\n",
=======
    ")  # Due to a bug in older versions of Langchain, JsonOutputParser did not stream results from some models\n",
>>>>>>> langchan/master
    "async for text in chain.astream(\n",
    "    \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n",
    "    'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n",
    "    \"Each country should have the key `name` and `population`\"\n",
    "):\n",
    "    print(text, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151d4323-a6cf-49be-8779-e8797c5e3b00",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐµÐ¼ ÑÐ»Ð¾Ð¼Ð°Ñ‚ÑŒ Ð¿Ð¾Ñ‚Ð¾ÐºÐ¾Ð²ÑƒÑŽ Ð¿ÐµÑ€ÐµÐ´Ð°Ñ‡Ñƒ.\n",
    "Ð”Ð»Ñ ÑÑ‚Ð¾Ð³Ð¾ Ð´Ð¾Ð±Ð°Ð²Ð¸Ð¼ Ð² ÐºÐ¾Ð½Ñ†Ðµ Ð¿Ñ€ÐµÐ´Ñ‹Ð´ÑƒÑ‰ÐµÐ³Ð¾ Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð° Ñ„ÑƒÐ½ÐºÑ†Ð¸ÑŽ, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ð¸Ð·Ð²Ð»ÐµÐºÐ°ÐµÑ‚ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ðµ ÑÑ‚Ñ€Ð°Ð½Ñ‹ Ð¸Ð· Ð¸Ñ‚Ð¾Ð³Ð¾Ð²Ð¾Ð³Ð¾ JSON.\n",
    "\n",
    ":::warning\n",
    "\n",
    "Ð›ÑŽÐ±Ð¾Ð¹ ÑÑ‚Ð°Ð¿ Ñ†ÐµÐ¿Ð¾Ñ‡ÐºÐ¸, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ñ Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½ÐµÐ½Ð½Ñ‹Ð¼Ð¸ Ð²Ñ…Ð¾Ð´Ð½Ñ‹Ð¼Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸, Ð° Ð½Ðµ Ñ Ð¿Ð¾Ñ‚Ð¾ÐºÐ¾Ð¼ Ð´Ð°Ð½Ð½Ñ‹Ñ…, Ð¼Ð¾Ð¶ÐµÑ‚ ÑÐ»Ð¾Ð¼Ð°Ñ‚ÑŒ Ð¿Ð¾Ñ‚Ð¾ÐºÐ¾Ð²ÑƒÑŽ Ð¿ÐµÑ€ÐµÐ´Ð°Ñ‡Ñƒ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Ð¼ÐµÑ‚Ð¾Ð´Ð¾Ð² `stream` Ð¸Ð»Ð¸ `astream`.\n",
    "\n",
    ":::\n",
    "\n",
    "<!--\n",
    ":::{.callout-tip}\n",
    "Later, we will discuss the `astream_events` API which streams results from intermediate steps. This API will stream results from intermediate steps even if the chain contains steps that only operate on **finalized inputs**.\n",
    ":::\n",
    "-->"
=======
    "Now, let's **break** streaming. We'll use the previous example and append an extraction function at the end that extracts the country names from the finalized JSON.\n",
    "\n",
    ":::{.callout-warning}\n",
    "Any steps in the chain that operate on **finalized inputs** rather than on **input streams** can break streaming functionality via `stream` or `astream`.\n",
    ":::\n",
    "\n",
    ":::{.callout-tip}\n",
    "Later, we will discuss the `astream_events` API which streams results from intermediate steps. This API will stream results from intermediate steps even if the chain contains steps that only operate on **finalized inputs**.\n",
    ":::"
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9c90117-9faa-4a01-b484-0db071808d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['France', 'Spain', 'Japan']|"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import (\n",
    "    JsonOutputParser,\n",
    ")\n",
    "\n",
    "\n",
<<<<<<< HEAD
    "# Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ Ð¾Ð±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÑ‚ Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½ÐµÐ½Ð½Ñ‹Ðµ Ð²Ñ…Ð¾Ð´Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ, Ð° Ð½Ðµ Ð¿Ð¾Ñ‚Ð¾Ðº input_stream\n",
    "def _extract_country_names(inputs):\n",
    "    \"\"\"Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ð½Ðµ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ñ Ð¿Ð¾Ñ‚Ð¾ÐºÐ¾Ð¼ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸ Ð»Ð¾Ð¼Ð°ÐµÑ‚ Ð¿Ð¾Ñ‚Ð¾ÐºÐ¾Ð²ÑƒÑŽ Ð¿ÐµÑ€ÐµÐ´Ð°Ñ‡Ñƒ.\"\"\"\n",
=======
    "# A function that operates on finalized inputs\n",
    "# rather than on an input_stream\n",
    "def _extract_country_names(inputs):\n",
    "    \"\"\"A function that does not operates on input streams and breaks streaming.\"\"\"\n",
>>>>>>> langchan/master
    "    if not isinstance(inputs, dict):\n",
    "        return \"\"\n",
    "\n",
    "    if \"countries\" not in inputs:\n",
    "        return \"\"\n",
    "\n",
    "    countries = inputs[\"countries\"]\n",
    "\n",
    "    if not isinstance(countries, list):\n",
    "        return \"\"\n",
    "\n",
    "    country_names = [\n",
    "        country.get(\"name\") for country in countries if isinstance(country, dict)\n",
    "    ]\n",
    "    return country_names\n",
    "\n",
    "\n",
    "chain = model | JsonOutputParser() | _extract_country_names\n",
    "\n",
    "async for text in chain.astream(\n",
    "    \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n",
    "    'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n",
    "    \"Each country should have the key `name` and `population`\"\n",
    "):\n",
    "    print(text, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab6dca2-2027-414d-a196-2db6e3ebb8a5",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "#### Ð¤ÑƒÐ½ÐºÑ†Ð¸Ð¸-Ð³ÐµÐ½ÐµÑ€Ð°Ñ‚Ð¾Ñ€Ñ‹\n",
    "\n",
    "Ð˜ÑÐ¿Ñ€Ð°Ð²Ð¸Ñ‚ÑŒ ÑÐ´ÐµÐ»Ð°Ð½Ð½Ñ‹Ðµ Ð²Ñ‹ÑˆÐµ Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ñ Ð¼Ð¾Ð¶Ð½Ð¾ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸-Ð³ÐµÐ½ÐµÑ€Ð°Ñ‚Ð¾Ñ€Ð°, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ñ Ð¿Ð¾Ñ‚Ð¾ÐºÐ¾Ð¼ Ð²Ñ…Ð¾Ð´Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ….\n",
    "\n",
    ":::note\n",
    "\n",
    "Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ-Ð³ÐµÐ½ÐµÑ€Ð°Ñ‚Ð¾Ñ€ Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ `yield` Ð¸ Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÑ‚ ÑÐ¾Ð·Ð´Ð°Ð²Ð°Ñ‚ÑŒ ÐºÐ¾Ð´, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ñ Ð¿Ð¾Ñ‚Ð¾ÐºÐ°Ð¼Ð¸ Ð²Ñ…Ð¾Ð´Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ….\n",
    "\n",
=======
    "#### Generator Functions\n",
    "\n",
    "Le'ts fix the streaming using a generator function that can operate on the **input stream**.\n",
    "\n",
    ":::{.callout-tip}\n",
    "A generator function (a function that uses `yield`) allows writing code that operates on **input streams**\n",
>>>>>>> langchan/master
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15984b2b-315a-4119-945b-2a3dabea3082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "France|Spain|Japan|"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "\n",
    "async def _extract_country_names_streaming(input_stream):\n",
<<<<<<< HEAD
    "    \"\"\"Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ñ Ð¿Ð¾Ñ‚Ð¾ÐºÐ¾Ð¼ Ð²Ñ…Ð¾Ð´Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ….\"\"\"\n",
=======
    "    \"\"\"A function that operates on input streams.\"\"\"\n",
>>>>>>> langchan/master
    "    country_names_so_far = set()\n",
    "\n",
    "    async for input in input_stream:\n",
    "        if not isinstance(input, dict):\n",
    "            continue\n",
    "\n",
    "        if \"countries\" not in input:\n",
    "            continue\n",
    "\n",
    "        countries = input[\"countries\"]\n",
    "\n",
    "        if not isinstance(countries, list):\n",
    "            continue\n",
    "\n",
    "        for country in countries:\n",
    "            name = country.get(\"name\")\n",
    "            if not name:\n",
    "                continue\n",
    "            if name not in country_names_so_far:\n",
    "                yield name\n",
    "                country_names_so_far.add(name)\n",
    "\n",
    "\n",
    "chain = model | JsonOutputParser() | _extract_country_names_streaming\n",
    "\n",
    "async for text in chain.astream(\n",
    "    \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n",
    "    'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n",
    "    \"Each country should have the key `name` and `population`\",\n",
    "):\n",
    "    print(text, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59823f5-9b9a-43c5-a213-34644e2f1d3d",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    ":::note\n",
    "\n",
    "Ð¦ÐµÐ¿Ð¾Ñ‡ÐºÐ° Ð¸Ð· Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð° Ð²Ñ‹ÑˆÐµ Ð¼Ð¾Ð¶ÐµÑ‚ Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°Ñ‚ÑŒ Ñ‡Ð°ÑÑ‚Ð¸ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ñ ÑÑ‚Ñ€Ð°Ð½Ñ‹.\n",
    "Ð­Ñ‚Ð¾ ÑÐ²ÑÐ·Ð°Ð½Ð½Ð¾ Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ð¾Ð¹ Ð°Ð²Ñ‚Ð¾Ð´Ð¾Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ Ð´Ð°Ð½Ð½Ñ‹Ñ… JSON.\n",
    "\n",
    "Ð”Ð»Ñ ÑÑ‚Ð¾Ð³Ð¾ Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð° ÑÑ‚Ð¾ Ð½ÐµÐ¿Ñ€Ð¸Ð½Ñ†Ð¸Ð¿Ð¸Ð°Ð»ÑŒÐ½Ð¾, Ñ‚Ð°Ðº ÐºÐ°Ðº ÐµÐ³Ð¾ Ñ†ÐµÐ»ÑŒ Ð¿Ñ€Ð¾Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿Ð¾Ð´Ñ…Ð¾Ð´Ñ‹ Ð² Ñ€Ð°Ð±Ð¾Ñ‚Ðµ Ñ Ð¿Ð¾Ñ‚Ð¾ÐºÐ¾Ð²Ð¾Ð¹ Ð¿ÐµÑ€ÐµÐ´Ð°Ñ‡ÐµÐ¹ Ð´Ð°Ð½Ð½Ñ‹Ñ….\n",
    "\n",
=======
    ":::{.callout-note}\n",
    "Because the code above is relying on JSON auto-completion, you may see partial names of countries (e.g., `Sp` and `Spain`), which is not what one would want for an extraction result!\n",
    "\n",
    "We're focusing on streaming concepts, not necessarily the results of the chains.\n",
>>>>>>> langchan/master
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adf65b7-aa47-4321-98c7-a0abe43b833a",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "### ÐšÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ñ‹ Ð±ÐµÐ· Ð¿Ð¾Ñ‚Ð¾ÐºÐ¾Ð²Ð¾Ð¹ Ð¿ÐµÑ€ÐµÐ´Ð°Ñ‡Ð¸\n",
    "\n",
    "ÐÐµÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ñ‹ GigaChain, Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, Ñ€ÐµÑ‚Ñ€Ð¸Ð²ÐµÑ€Ñ‹, Ð½Ðµ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÑŽÑ‚ Ñ€Ð°Ð±Ð¾Ñ‚Ñƒ Ñ Ð¿Ð¾Ñ‚Ð¾ÐºÐ¾Ð¼ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð².\n",
    "\n",
    "ÐÐ¸Ð¶Ðµ Ð¿Ð¾ÐºÐ°Ð·Ð°Ð½Ð¾, Ñ‡Ñ‚Ð¾ ÑÐ»ÑƒÑ‡Ð¸Ñ‚ÑÑ ÐµÑÐ»Ð¸ Ð²Ñ‹Ð·Ð²Ð°Ñ‚ÑŒ Ð¼ÐµÑ‚Ð¾Ð´ `stream` Ð² Ñ‚Ð°ÐºÐ¸Ñ… ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ð°Ñ…."
=======
    "### Non-streaming components\n",
    "\n",
    "Some built-in components like Retrievers do not offer any `streaming`. What happens if we try to `stream` them? ðŸ¤¨"
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9b1c00d-8b44-40d0-9e2b-8a70d238f82b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(page_content='harrison worked at kensho'),\n",
       "  Document(page_content='harrison likes spicy food')]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"harrison worked at kensho\", \"harrison likes spicy food\"],\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "chunks = [chunk for chunk in retriever.stream(\"where did harrison work?\")]\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd3e71b-439e-418f-8a8a-5232fba3d9fd",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "ÐœÐµÑ‚Ð¾Ð´ Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ð²Ñ‹Ð²Ð¾Ð´ ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ð° Ñ†ÐµÐ»Ð¸ÐºÐ¾Ð¼.\n",
    "Ð­Ñ‚Ð¾ Ð½Ð¾Ñ€Ð¼Ð°Ð»ÑŒÐ½Ð¾Ðµ Ð¿Ð¾Ð²ÐµÐ´ÐµÐ½Ð¸Ðµ Ð´Ð»Ñ ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ð¾Ð², ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð½Ðµ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÑŽÑ‚ Ñ€Ð°Ð±Ð¾Ñ‚Ñƒ Ñ Ð¿Ð¾Ñ‚Ð¾ÐºÐ¾Ð¼ Ð´Ð°Ð½Ð½Ñ‹Ñ….\n",
    "\n",
    ":::note\n",
    "\n",
    "Ð’ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ð½ÑÑ‚Ð²Ðµ ÑÐ»ÑƒÑ‡Ð°ÐµÐ² LCEL-Ñ†ÐµÐ¿Ð¾Ñ‡ÐºÐ°, Ð½ÐµÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ñ‹ ÐºÐ¾Ñ‚Ð¾Ñ€Ð¾Ð¹ Ð½Ðµ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÑŽÑ‚ Ð¿Ð¾Ñ‚Ð¾ÐºÐ¾Ð²ÑƒÑŽ Ð¿ÐµÑ€ÐµÐ´Ð°Ñ‡Ñƒ, Ð²ÑÐµ Ñ€Ð°Ð²Ð½Ð¾ Ð±ÑƒÐ´ÐµÑ‚ Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ Ñ Ð¿Ð¾Ñ‚Ð¾ÐºÐ¾Ð¼ Ð´Ð°Ð½Ð½Ñ‹Ñ….\n",
    "Ð’ Ñ‚Ð°ÐºÐ¸Ñ… ÑÐ»ÑƒÑ‡Ð°ÑÑ… Ð¿Ð¾Ñ‚Ð¾ÐºÐ¾Ð²Ð°Ñ Ð¿ÐµÑ€ÐµÐ´Ð°Ñ‡Ð° Ð²Ð¾Ð·Ð¾Ð±Ð½Ð¾Ð²Ð»ÑÐµÑ‚ÑÑ Ð¿Ð¾ÑÐ»Ðµ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½ÐµÐ³Ð¾ ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ð°, Ð½Ðµ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÑŽÑ‰ÐµÐ³Ð¾ Ð¿Ð¾Ñ‚Ð¾ÐºÐ¾Ð²Ñ‹Ð¹ Ð²Ñ‹Ð²Ð¾Ð´ Ð´Ð°Ð½Ð½Ñ‹Ñ….\n",
    "\n",
=======
    "Stream just yielded the final result from that component.\n",
    "\n",
    "This is OK ðŸ¥¹! Not all components have to implement streaming -- in some cases streaming is either unnecessary, difficult or just doesn't make sense.\n",
    "\n",
    ":::{.callout-tip}\n",
    "An LCEL chain constructed using non-streaming components, will still be able to stream in a lot of cases, with streaming of partial output starting after the last non-streaming step in the chain.\n",
>>>>>>> langchan/master
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "957447e6-1e60-41ef-8c10-2654bd9e738d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain = (\n",
    "    {\n",
    "        \"context\": retriever.with_config(run_name=\"Docs\"),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94e50b5d-bf51-4eee-9da0-ee40dd9ce42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base|d on| the| given| context|,| Harrison| worke|d at| K|ens|ho|.|\n",
      "\n",
      "Here| are| |3| |made| up| sentences| about| this| place|:|\n",
      "\n",
      "1|.| K|ens|ho| was| a| cutting|-|edge| technology| company| known| for| its| innovative| solutions| in| artificial| intelligence| an|d data| analytics|.|\n",
      "\n",
      "2|.| The| modern| office| space| at| K|ens|ho| feature|d open| floor| plans|,| collaborative| work|sp|aces|,| an|d a| vib|rant| atmosphere| that| fos|tere|d creativity| an|d team|work|.|\n",
      "\n",
      "3|.| With| its| prime| location| in| the| heart| of| the| city|,| K|ens|ho| attracte|d top| talent| from| aroun|d the| worl|d,| creating| a| diverse| an|d dynamic| work| environment|.|"
     ]
    }
   ],
   "source": [
    "for chunk in retrieval_chain.stream(\n",
    "    \"Where did harrison work? \" \"Write 3 made up sentences about this place.\"\n",
    "):\n",
    "    print(chunk, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "baceb5c0-d4a4-4b98-8733-80ae4407b62d",
   "metadata": {},
   "source": [
    "## ÐŸÐ¾Ñ‚Ð¾ÐºÐ¾Ð²Ð°Ñ Ð¿ÐµÑ€ÐµÐ´Ð°Ñ‡Ð° ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ð¹\n",
    "\n",
    "Ð Ð°Ð±Ð¾Ñ‚Ð° Ñ Ð¿Ð¾Ñ‚Ð¾ÐºÐ¾Ð¼ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ð¹ Ð±Ñ‹Ð»Ð° Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð° Ð² Ð²ÐµÑ€ÑÐ¸Ð¸ Ð±Ð¸Ð±Ð»Ð¸Ð¾Ñ‚ÐµÐºÐ¸ langchain-core **0.1.14**.\n",
    "\n",
    "Ð¡ÐµÐ¹Ñ‡Ð°Ñ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð° Ð±ÐµÑ‚Ð°-Ð²ÐµÑ€ÑÐ¸Ñ API, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ð¼Ð¾Ð¶ÐµÑ‚ Ð¼ÐµÐ½ÑÑ‚ÑŒÑÑ."
=======
   "id": "8657aa4e-3469-4b5b-a09c-60b53a23b1e7",
   "metadata": {},
   "source": [
    "Now that we've seen how `stream` and `astream` work, let's venture into the world of streaming events. ðŸžï¸"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baceb5c0-d4a4-4b98-8733-80ae4407b62d",
   "metadata": {},
   "source": [
    "## Using Stream Events\n",
    "\n",
    "Event Streaming is a **beta** API. This API may change a bit based on feedback.\n",
    "\n",
    ":::{.callout-note}\n",
    "\n",
    "This guide demonstrates the `V2` API and requires langchain-core >= 0.2. For the `V1` API compatible with older versions of LangChain, see [here](https://python.langchain.com/v0.1/docs/expression_language/streaming/#using-stream-events).\n",
    ":::"
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61348df9-ec58-401e-be89-68a70042f88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain_core\n",
    "\n",
    "langchain_core.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e9e983-bbde-4906-9eca-4ccc06eabd91",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "Ð”Ð»Ñ ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ð¾Ð¹ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ API `astream_events`:\n",
    "\n",
    "* Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ `async`, Ð³Ð´Ðµ ÑÑ‚Ð¾ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾, Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, Ð¿Ñ€Ð¸ Ð²Ñ‹Ð·Ð¾Ð²Ðµ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð².\n",
    "* Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐ¹Ñ‚Ðµ Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ñ‹Ðµ Ð²Ñ‹Ð·Ð¾Ð²Ñ‹ Ð¿Ñ€Ð¸ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ð¸ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÑÐºÐ¸Ñ… Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¹ Ð¸Ð»Ð¸ Runnable.\n",
    "* Ð•ÑÐ»Ð¸ Ð²Ñ‹ Ð¸ÑÐ¿Ñ€Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚Ðµ Runnable Ð±ÐµÐ· LCEL, Ð²Ñ‹Ð·Ñ‹Ð²Ð°Ð¹Ñ‚Ðµ Ð¼ÐµÑ‚Ð¾Ð´ `.astream()` Ð¿Ñ€Ð¸ Ñ€Ð°Ð±Ð¾Ñ‚Ðµ Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒÑŽ, Ð²Ð¼ÐµÑÑ‚Ð¾ `.ainvoke`. Ð­Ñ‚Ð¾ Ð¿Ð¾Ð·Ð²Ð¾Ð»Ð¸Ñ‚ Ð·Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚ÑŒ Ð¿Ð¾Ñ‚Ð¾ÐºÐ¾Ð²ÑƒÑŽ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸ÑŽ Ð¿Ñ€Ð¸Ð½ÑƒÐ´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾.\n",
    "\n",
    "### ÐžÐ¿Ð¸ÑÐ°Ð½Ð¸Ðµ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ð¹\n",
    "\n",
    "Ð’ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ðµ Ð½Ð¸Ð¶Ðµ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ñ‹ Ð½ÐµÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ñ, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð¼Ð¾Ð³ÑƒÑ‚ Ð±Ñ‹Ñ‚ÑŒ ÑÐ³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ñ‹ Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ð¼Ð¸ Ð¾Ð±ÑŠÐµÐºÑ‚Ð°Ð¼Ð¸ Runnable.\n",
    "\n",
    ":::note\n",
    "\n",
    "ÐŸÑ€Ð¸ Ð¿Ð¾Ñ‚Ð¾ÐºÐ¾Ð²Ð¾Ð¹ Ð¿ÐµÑ€ÐµÐ´Ð°Ñ‡Ðµ Ð¿Ð¾Ñ‚Ð¾ÐºÐ° Ð²Ñ…Ð¾Ð´Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð² Runnable-Ð¾Ð±ÑŠÐµÐºÑ‚, Ð¾Ð½Ð¸ Ð½Ðµ Ð±ÑƒÐ´ÑƒÑ‚ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹ Ð´Ð¾ Ñ‚ÐµÑ… Ð¿Ð¾Ñ€, Ð¿Ð¾ÐºÐ° Ð¿Ð¾Ñ‚Ð¾Ðº Ð½Ðµ Ð±ÑƒÐ´ÐµÑ‚ Ð¿Ð¾Ñ‚Ñ€ÐµÐ±Ð»ÐµÐ½ Ð¿Ð¾Ð»Ð½Ð¾ÑÑ‚ÑŒÑŽ.\n",
    "Ð­Ñ‚Ð¾ Ð¾Ð·Ð½Ð°Ñ‡Ð°ÐµÑ‚, Ñ‡Ñ‚Ð¾ Ð²Ñ…Ð¾Ð´Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð±ÑƒÐ´ÑƒÑ‚ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹ Ð½Ð° ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‰ÐµÐ¼ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ð¸ `end`, Ð° Ð½Ðµ Ð½Ð° ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ð¸ `start`.\n",
    "\n",
    ":::\n",
    "\n",
    "| ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ðµ                  | Ð¸Ð¼Ñ             | ÐºÑƒÑÐ¾Ðº                           | Ð²Ñ…Ð¾Ð´                                         | Ð²Ñ‹Ñ…Ð¾Ð´                                          |\n",
    "|--------------------------|-----------------|---------------------------------|----------------------------------------------|------------------------------------------------|\n",
    "| on_chat_model_start      | [Ð¸Ð¼Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸]    |                                 | {\"ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ\": [[SystemMessage, HumanMessage]]} |                                                |\n",
    "| on_chat_model_stream     | [Ð¸Ð¼Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸]    | AIMessageChunk(content=\"hello\") |                                              |                                                |\n",
    "| on_chat_model_end        | [Ð¸Ð¼Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸]    |                                 | {\"ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ\": [[SystemMessage, HumanMessage]]} | {\"Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸\": [...], \"Ð²Ñ‹Ñ…Ð¾Ð´ llm\": None, ...}   |\n",
    "| on_llm_start             | [Ð¸Ð¼Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸]    |                                 | {'Ð²Ñ…Ð¾Ð´': 'Ð¿Ñ€Ð¸Ð²ÐµÑ‚'}                           |                                                |\n",
    "| on_llm_stream            | [Ð¸Ð¼Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸]    | 'ÐŸÑ€Ð¸Ð²ÐµÑ‚'                        |                                              |                                                |\n",
    "| on_llm_end               | [Ð¸Ð¼Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸]    |                                 | 'ÐŸÑ€Ð¸Ð²ÐµÑ‚, Ñ‡ÐµÐ»Ð¾Ð²ÐµÐº!'                           |\n",
    "| on_chain_start           | format_docs     |                                 |                                              |                                                |\n",
    "| on_chain_stream          | format_docs     | \"Ð¿Ñ€Ð¸Ð²ÐµÑ‚ Ð¼Ð¸Ñ€!, Ð¿Ñ€Ð¾Ñ‰Ð°Ð¹ Ð¼Ð¸Ñ€!\"      |                                              |                                                |\n",
    "| on_chain_end             | format_docs     |                                 | [Ð”Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚(...)]                              | \"Ð¿Ñ€Ð¸Ð²ÐµÑ‚ Ð¼Ð¸Ñ€!, Ð¿Ñ€Ð¾Ñ‰Ð°Ð¹ Ð¼Ð¸Ñ€!\"                      |\n",
    "| on_tool_start            | some_tool       |                                 | {\"x\": 1, \"y\": \"2\"}                           |                                                |\n",
    "| on_tool_stream           | some_tool       | {\"x\": 1, \"y\": \"2\"}              |                                              |                                                |\n",
    "| on_tool_end              | some_tool       |                                 |                                              | {\"x\": 1, \"y\": \"2\"}                             |\n",
    "| on_retriever_start       | [Ð¸Ð¼Ñ Ñ€ÐµÑ‚Ñ€Ð¸Ð²ÐµÑ€Ð°] |                                 | {\"Ð·Ð°Ð¿Ñ€Ð¾Ñ\": \"Ð¿Ñ€Ð¸Ð²ÐµÑ‚\"}                         |                                                |\n",
    "| on_retriever_chunk       | [Ð¸Ð¼Ñ Ñ€ÐµÑ‚Ñ€Ð¸Ð²ÐµÑ€Ð°] | {Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹: [...]}              |                                              |                                                |\n",
    "| on_retriever_end         | [Ð¸Ð¼Ñ Ñ€ÐµÑ‚Ñ€Ð¸Ð²ÐµÑ€Ð°] |                                 | {\"Ð·Ð°Ð¿Ñ€Ð¾Ñ\": \"Ð¿Ñ€Ð¸Ð²ÐµÑ‚\"}                         | {Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹: [...]}                             |\n",
    "| on_prompt_start          | [Ð¸Ð¼Ñ ÑˆÐ°Ð±Ð»Ð¾Ð½Ð°]   |                                 | {\"Ð²Ð¾Ð¿Ñ€Ð¾Ñ\": \"Ð¿Ñ€Ð¸Ð²ÐµÑ‚\"}                         |                                                |\n",
    "| on_prompt_end            | [Ð¸Ð¼Ñ ÑˆÐ°Ð±Ð»Ð¾Ð½Ð°]   |                                 | {\"Ð²Ð¾Ð¿Ñ€Ð¾Ñ\": \"Ð¿Ñ€Ð¸Ð²ÐµÑ‚\"}                         | ChatPromptValue(ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ: [SystemMessage, ...])|"
=======
    "For the `astream_events` API to work properly:\n",
    "\n",
    "* Use `async` throughout the code to the extent possible (e.g., async tools etc)\n",
    "* Propagate callbacks if defining custom functions / runnables\n",
    "* Whenever using runnables without LCEL, make sure to call `.astream()` on LLMs rather than `.ainvoke` to force the LLM to stream tokens.\n",
    "* Let us know if anything doesn't work as expected! :)\n",
    "\n",
    "### Event Reference\n",
    "\n",
    "Below is a reference table that shows some events that might be emitted by the various Runnable objects.\n",
    "\n",
    "\n",
    ":::{.callout-note}\n",
    "When streaming is implemented properly, the inputs to a runnable will not be known until after the input stream has been entirely consumed. This means that `inputs` will often be included only for `end` events and rather than for `start` events.\n",
    ":::\n",
    "\n",
    "| event                | name             | chunk                           | input                                         | output                                          |\n",
    "|----------------------|------------------|---------------------------------|-----------------------------------------------|-------------------------------------------------|\n",
    "| on_chat_model_start  | [model name]     |                                 | {\"messages\": [[SystemMessage, HumanMessage]]} |                                                 |\n",
    "| on_chat_model_stream | [model name]     | AIMessageChunk(content=\"hello\") |                                               |                                                 |\n",
    "| on_chat_model_end    | [model name]     |                                 | {\"messages\": [[SystemMessage, HumanMessage]]} | AIMessageChunk(content=\"hello world\")           |\n",
    "| on_llm_start         | [model name]     |                                 | {'input': 'hello'}                            |                                                 |\n",
    "| on_llm_stream        | [model name]     | 'Hello'                         |                                               |                                                 |\n",
    "| on_llm_end           | [model name]     |                                 | 'Hello human!'                                |                                                 |\n",
    "| on_chain_start       | format_docs      |                                 |                                               |                                                 |\n",
    "| on_chain_stream      | format_docs      | \"hello world!, goodbye world!\"  |                                               |                                                 |\n",
    "| on_chain_end         | format_docs      |                                 | [Document(...)]                               | \"hello world!, goodbye world!\"                  |\n",
    "| on_tool_start        | some_tool        |                                 | {\"x\": 1, \"y\": \"2\"}                            |                                                 |\n",
    "| on_tool_end          | some_tool        |                                 |                                               | {\"x\": 1, \"y\": \"2\"}                              |\n",
    "| on_retriever_start   | [retriever name] |                                 | {\"query\": \"hello\"}                            |                                                 |\n",
    "| on_retriever_end     | [retriever name] |                                 | {\"query\": \"hello\"}                            | [Document(...), ..]                             |\n",
    "| on_prompt_start      | [template_name]  |                                 | {\"question\": \"hello\"}                         |                                                 |\n",
    "| on_prompt_end        | [template_name]  |                                 | {\"question\": \"hello\"}                         | ChatPromptValue(messages: [SystemMessage, ...]) |"
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6ec135-3348-4041-8f55-bf3e59b3b2d0",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "### Ð§Ð°Ñ‚-Ð¼Ð¾Ð´ÐµÐ»ÑŒ\n",
    "\n",
    "ÐŸÐ¾ÑÐ¼Ð¾Ñ‚Ñ€Ð¸Ð¼ ÐºÐ°ÐºÐ¸Ðµ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ñ Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ñ‡Ð°Ñ‚-Ð¼Ð¾Ð´ÐµÐ»ÑŒ."
=======
    "### Chat Model\n",
    "\n",
    "Let's start off by looking at the events produced by a chat model."
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c00df46e-7f6b-4e06-8abf-801898c8d57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eugene/src/langchain/libs/core/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: This API is in beta and may change in the future.\n",
      "  warn_beta(\n"
     ]
    }
   ],
   "source": [
    "events = []\n",
    "async for event in model.astream_events(\"hello\", version=\"v2\"):\n",
    "    events.append(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32972939-2995-4b2e-84db-045adb044fad",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "::: note\n",
    "\n",
    "ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€ Ð²ÐµÑ€ÑÐ¸Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ Ð¿Ð¾Ñ‚Ð¾Ð¼Ñƒ, Ñ‡Ñ‚Ð¾ API Ð½Ð°Ñ…Ð¾Ð´Ð¸Ñ‚ÑÑ Ð² Ð±ÐµÑ‚Ð°-Ð²ÐµÑ€ÑÐ¸Ð¸.\n",
    "ÐžÐ½ Ð¿Ð¾Ð·Ð²Ð¾Ð»Ð¸Ñ‚ Ð¼Ð¸Ð½Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð¼Ð¾Ð³ÑƒÑ‚ Ð²Ð¾Ð·Ð½Ð¸ÐºÐ½ÑƒÑ‚ÑŒ Ð¿Ñ€Ð¸ Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ð¸ API.\n",
=======
    ":::{.callout-note}\n",
    "\n",
    "Hey what's that funny version=\"v2\" parameter in the API?! ðŸ˜¾\n",
    "\n",
    "This is a **beta API**, and we're almost certainly going to make some changes to it (in fact, we already have!)\n",
    "\n",
    "This version parameter will allow us to minimize such breaking changes to your code. \n",
    "\n",
    "In short, we are annoying you now, so we don't have to annoy you later.\n",
    "\n",
    "`v2` is only available for langchain-core>=0.2.0.\n",
>>>>>>> langchan/master
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2b8f47-da78-4569-a49a-53a8efaa26bc",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "Ð Ð°ÑÑÐ¼Ð¾Ñ‚Ñ€Ð¸Ð¼ Ð½ÐµÑÐ¾ÐºÐ»ÑŒÐºÐ¾ Ð½Ð°Ñ‡Ð°Ð»ÑŒÐ½Ñ‹Ñ… (`start`) Ð¸ ÐºÐ¾Ð½ÐµÑ‡Ð½Ñ‹Ñ… (`end`) ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ð¹."
=======
    "Let's take a look at the few of the start event and a few of the end events."
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce31b525-f47d-4828-85a7-912ce9f2e79b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'event': 'on_chat_model_start',\n",
       "  'data': {'input': 'hello'},\n",
       "  'name': 'ChatAnthropic',\n",
       "  'tags': [],\n",
       "  'run_id': 'a81e4c0f-fc36-4d33-93bc-1ac25b9bb2c3',\n",
       "  'metadata': {}},\n",
       " {'event': 'on_chat_model_stream',\n",
       "  'data': {'chunk': AIMessageChunk(content='Hello', id='run-a81e4c0f-fc36-4d33-93bc-1ac25b9bb2c3')},\n",
       "  'run_id': 'a81e4c0f-fc36-4d33-93bc-1ac25b9bb2c3',\n",
       "  'name': 'ChatAnthropic',\n",
       "  'tags': [],\n",
       "  'metadata': {}},\n",
       " {'event': 'on_chat_model_stream',\n",
       "  'data': {'chunk': AIMessageChunk(content='!', id='run-a81e4c0f-fc36-4d33-93bc-1ac25b9bb2c3')},\n",
       "  'run_id': 'a81e4c0f-fc36-4d33-93bc-1ac25b9bb2c3',\n",
       "  'name': 'ChatAnthropic',\n",
       "  'tags': [],\n",
       "  'metadata': {}}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76cfe826-ee63-4310-ad48-55a95eb3b9d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'event': 'on_chat_model_stream',\n",
       "  'data': {'chunk': AIMessageChunk(content='?', id='run-a81e4c0f-fc36-4d33-93bc-1ac25b9bb2c3')},\n",
       "  'run_id': 'a81e4c0f-fc36-4d33-93bc-1ac25b9bb2c3',\n",
       "  'name': 'ChatAnthropic',\n",
       "  'tags': [],\n",
       "  'metadata': {}},\n",
       " {'event': 'on_chat_model_end',\n",
       "  'data': {'output': AIMessageChunk(content='Hello! How can I assist you today?', id='run-a81e4c0f-fc36-4d33-93bc-1ac25b9bb2c3')},\n",
       "  'run_id': 'a81e4c0f-fc36-4d33-93bc-1ac25b9bb2c3',\n",
       "  'name': 'ChatAnthropic',\n",
       "  'tags': [],\n",
       "  'metadata': {}}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events[-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c8f173-e9c7-4c27-81a5-b7c85c12714d",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "### Ð Ð°Ð±Ð¾Ñ‚Ð° Ñ Ñ†ÐµÐ¿Ð¾Ñ‡ÐºÐ¾Ð¹\n",
    "\n",
    "Ð”Ð»Ñ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð°Ñ†Ð¸Ð¸ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ API ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ð¹ Ð¾Ð±Ñ€Ð°Ñ‚Ð¸Ð¼ÑÑ Ðº Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ñƒ Ñ†ÐµÐ¿Ð¾Ñ‡ÐºÐ¸, Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð²ÑˆÐµÐ¼Ñƒ Ñ Ð¿Ð¾Ñ‚Ð¾ÐºÐ¾Ð²Ð¾Ð¹ Ð¿ÐµÑ€ÐµÐ´Ð°Ñ‡ÐµÐ¹ JSON."
=======
    "### Chain\n",
    "\n",
    "Let's revisit the example chain that parsed streaming JSON to explore the streaming events API."
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4328c56c-a303-427b-b1f2-f354e9af555c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    model | JsonOutputParser()\n",
    ")  # Due to a bug in older versions of Langchain, JsonOutputParser did not stream results from some models\n",
    "\n",
    "events = [\n",
    "    event\n",
    "    async for event in chain.astream_events(\n",
    "        \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n",
    "        'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n",
    "        \"Each country should have the key `name` and `population`\",\n",
    "        version=\"v2\",\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc00b99-a961-4221-a3c7-9d807114bbfb",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "ÐœÐ¾Ð¶Ð½Ð¾ Ð·Ð°Ð¼ÐµÑ‚Ð¸Ñ‚ÑŒ, Ñ‡Ñ‚Ð¾ Ð²Ð¼ÐµÑÑ‚Ð¾ Ð´Ð²ÑƒÑ… Ð½Ð°Ñ‡Ð°Ð»ÑŒÐ½Ñ‹Ñ… ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ð¹ Ð¼Ñ‹ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ð»Ð¸ Ñ‚Ñ€Ð¸.\n",
    "\n",
    "Ð¢Ñ€Ð¸ Ð½Ð°Ñ‡Ð°Ð»ÑŒÐ½Ñ‹Ñ… ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ñ ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÑƒÑŽ ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ð°Ð¼:\n",
    "\n",
    "1. Ð¦ÐµÐ¿Ð¾Ñ‡ÐºÐ° (Ð¼Ð¾Ð´ÐµÐ»ÑŒ + Ð¿Ð°Ñ€ÑÐµÑ€)\n",
    "2. ÐœÐ¾Ð´ÐµÐ»ÑŒ.\n",
    "3. ÐŸÐ°Ñ€ÑÐµÑ€."
=======
    "If you examine at the first few events, you'll notice that there are **3** different start events rather than **2** start events.\n",
    "\n",
    "The three start events correspond to:\n",
    "\n",
    "1. The chain (model + parser)\n",
    "2. The model\n",
    "3. The parser"
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e66ea3d-a450-436a-aaac-d9478abc6c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'event': 'on_chain_start',\n",
       "  'data': {'input': 'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`'},\n",
       "  'name': 'RunnableSequence',\n",
       "  'tags': [],\n",
       "  'run_id': '4765006b-16e2-4b1d-a523-edd9fd64cb92',\n",
       "  'metadata': {}},\n",
       " {'event': 'on_chat_model_start',\n",
       "  'data': {'input': {'messages': [[HumanMessage(content='output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`')]]}},\n",
       "  'name': 'ChatAnthropic',\n",
       "  'tags': ['seq:step:1'],\n",
       "  'run_id': '0320c234-7b52-4a14-ae4e-5f100949e589',\n",
       "  'metadata': {}},\n",
       " {'event': 'on_chat_model_stream',\n",
       "  'data': {'chunk': AIMessageChunk(content='{', id='run-0320c234-7b52-4a14-ae4e-5f100949e589')},\n",
       "  'run_id': '0320c234-7b52-4a14-ae4e-5f100949e589',\n",
       "  'name': 'ChatAnthropic',\n",
       "  'tags': ['seq:step:1'],\n",
       "  'metadata': {}}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events[:3]"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "c742cfa4-9b03-4a5b-96d9-5fe56e95e3b4",
   "metadata": {},
   "source": [
    "Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð½Ñ‹Ð¹ API Ð´Ð»Ñ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ñ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ð¹ Ð¾Ñ‚ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¸ Ð¿Ð°Ñ€ÑÐµÑ€Ð°.\n",
    "ÐÐµ Ð±ÑƒÐ´ÐµÐ¼ Ð¾Ð±Ñ€Ð°Ñ‰Ð°Ñ‚ÑŒ Ð²Ð½Ð¸Ð¼Ð°Ð½Ð¸Ñ Ð½Ð° Ð½Ð°Ñ‡Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð¸ ÐºÐ¾Ð½ÐµÑ‡Ð½Ñ‹Ðµ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ñ, Ð° Ñ‚Ð°ÐºÐ¶Ðµ Ð½Ð° ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ñ Ñ†ÐµÐ¿Ð¾Ñ‡ÐºÐ¸."
=======
   "id": "c8512238-d035-4acd-9248-a8570da064c9",
   "metadata": {},
   "source": [
    "What do you think you'd see if you looked at the last 3 events? what about the middle?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c742cfa4-9b03-4a5b-96d9-5fe56e95e3b4",
   "metadata": {},
   "source": [
    "Let's use this API to take output the stream events from the model and the parser. We're ignoring start events, end events and events from the chain."
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "630c71d6-8d94-4ce0-a78a-f20e90f628df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat model chunk: '{'\n",
      "Parser chunk: {}\n",
      "Chat model chunk: '\\n  '\n",
      "Chat model chunk: '\"'\n",
      "Chat model chunk: 'countries'\n",
      "Chat model chunk: '\":'\n",
      "Chat model chunk: ' ['\n",
      "Parser chunk: {'countries': []}\n",
      "Chat model chunk: '\\n    '\n",
      "Chat model chunk: '{'\n",
      "Parser chunk: {'countries': [{}]}\n",
      "Chat model chunk: '\\n      '\n",
      "Chat model chunk: '\"'\n",
      "Chat model chunk: 'name'\n",
      "Chat model chunk: '\":'\n",
      "Chat model chunk: ' \"'\n",
      "Parser chunk: {'countries': [{'name': ''}]}\n",
      "Chat model chunk: 'France'\n",
      "Parser chunk: {'countries': [{'name': 'France'}]}\n",
      "Chat model chunk: '\",'\n",
      "Chat model chunk: '\\n      '\n",
      "Chat model chunk: '\"'\n",
      "Chat model chunk: 'population'\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "num_events = 0\n",
    "\n",
    "async for event in chain.astream_events(\n",
    "    \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n",
    "    'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n",
    "    \"Each country should have the key `name` and `population`\",\n",
    "    version=\"v2\",\n",
    "):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        print(\n",
    "            f\"Chat model chunk: {repr(event['data']['chunk'].content)}\",\n",
    "            flush=True,\n",
    "        )\n",
    "    if kind == \"on_parser_stream\":\n",
    "        print(f\"Parser chunk: {event['data']['chunk']}\", flush=True)\n",
    "    num_events += 1\n",
    "    if num_events > 30:\n",
    "        # Truncate the output\n",
    "        print(\"...\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798ea891-997c-454c-bf60-43124f40ee1b",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "ÐœÑ‹ Ð¼Ð¾Ð¶ÐµÐ¼ Ð¿Ð¾Ð»ÑƒÑ‡Ð°Ñ‚ÑŒ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ñ Ð¾Ð±Ð¾Ð¸Ñ… ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ð¾Ð² Ð¾Ð´Ð½Ð¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾, Ð¿Ð¾Ñ‚Ð¾Ð¼Ñƒ Ñ‡Ñ‚Ð¾ ÐºÐ°Ð¶Ð´Ñ‹Ð¹ Ð¸Ð· Ð½Ð¸Ñ… Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÐµÑ‚ Ñ€Ð°Ð±Ð¾Ñ‚Ñƒ Ñ Ð¿Ð¾Ñ‚Ð¾ÐºÐ¾Ð¼ Ð´Ð°Ð½Ð½Ñ‹Ñ…."
=======
    "Because both the model and the parser support streaming, we see sreaming events from both components in real time! Kind of cool isn't it? ðŸ¦œ"
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5084148b-bcdc-4373-9caa-6568f03e7b23",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "### Ð¤Ð¸Ð»ÑŒÑ‚Ñ€Ð°Ñ†Ð¸Ñ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ð¹\n",
    "\n",
    "API Ñ‚Ñ€Ð°Ð½ÑÐ»Ð¸Ñ€ÑƒÐµÑ‚ Ð¾Ñ‡ÐµÐ½ÑŒ Ð¼Ð½Ð¾Ð³Ð¾ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ð¹.\n",
    "Ð¡Ð¾Ð±Ñ‹Ñ‚Ð¸Ñ Ð¼Ð¾Ð¶Ð½Ð¾ Ð¾Ñ‚Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿Ð¾ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸ÑŽ (`name`), Ñ‚ÐµÐ³Ð°Ð¼ (`tags`) Ð¸Ð»Ð¸ Ñ‚Ð¸Ð¿Ñƒ (`type`) ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ð°.\n",
    "\n",
    "#### ÐŸÐ¾ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸ÑŽ"
=======
    "### Filtering Events\n",
    "\n",
    "Because this API produces so many events, it is useful to be able to filter on events.\n",
    "\n",
    "You can filter by either component `name`, component `tags` or component `type`.\n",
    "\n",
    "#### By Name"
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f0b581b-be63-4663-baba-c6d2b625cdf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'event': 'on_parser_start', 'data': {'input': 'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`'}, 'name': 'my_parser', 'tags': ['seq:step:2'], 'run_id': 'e058d750-f2c2-40f6-aa61-10f84cd671a9', 'metadata': {}}\n",
      "{'event': 'on_parser_stream', 'data': {'chunk': {}}, 'run_id': 'e058d750-f2c2-40f6-aa61-10f84cd671a9', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}}\n",
      "{'event': 'on_parser_stream', 'data': {'chunk': {'countries': []}}, 'run_id': 'e058d750-f2c2-40f6-aa61-10f84cd671a9', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}}\n",
      "{'event': 'on_parser_stream', 'data': {'chunk': {'countries': [{}]}}, 'run_id': 'e058d750-f2c2-40f6-aa61-10f84cd671a9', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}}\n",
      "{'event': 'on_parser_stream', 'data': {'chunk': {'countries': [{'name': ''}]}}, 'run_id': 'e058d750-f2c2-40f6-aa61-10f84cd671a9', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}}\n",
      "{'event': 'on_parser_stream', 'data': {'chunk': {'countries': [{'name': 'France'}]}}, 'run_id': 'e058d750-f2c2-40f6-aa61-10f84cd671a9', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}}\n",
      "{'event': 'on_parser_stream', 'data': {'chunk': {'countries': [{'name': 'France', 'population': 67}]}}, 'run_id': 'e058d750-f2c2-40f6-aa61-10f84cd671a9', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}}\n",
      "{'event': 'on_parser_stream', 'data': {'chunk': {'countries': [{'name': 'France', 'population': 67413}]}}, 'run_id': 'e058d750-f2c2-40f6-aa61-10f84cd671a9', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}}\n",
      "{'event': 'on_parser_stream', 'data': {'chunk': {'countries': [{'name': 'France', 'population': 67413000}]}}, 'run_id': 'e058d750-f2c2-40f6-aa61-10f84cd671a9', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}}\n",
      "{'event': 'on_parser_stream', 'data': {'chunk': {'countries': [{'name': 'France', 'population': 67413000}, {}]}}, 'run_id': 'e058d750-f2c2-40f6-aa61-10f84cd671a9', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}}\n",
      "{'event': 'on_parser_stream', 'data': {'chunk': {'countries': [{'name': 'France', 'population': 67413000}, {'name': ''}]}}, 'run_id': 'e058d750-f2c2-40f6-aa61-10f84cd671a9', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}}\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "chain = model.with_config({\"run_name\": \"model\"}) | JsonOutputParser().with_config(\n",
    "    {\"run_name\": \"my_parser\"}\n",
    ")\n",
    "\n",
    "max_events = 0\n",
    "async for event in chain.astream_events(\n",
    "    \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n",
    "    'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n",
    "    \"Each country should have the key `name` and `population`\",\n",
    "    version=\"v2\",\n",
    "    include_names=[\"my_parser\"],\n",
    "):\n",
    "    print(event)\n",
    "    max_events += 1\n",
    "    if max_events > 10:\n",
    "        # Truncate output\n",
    "        print(\"...\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59d5626-7dba-4eb3-ad81-76c1092c5146",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "#### ÐŸÐ¾ Ñ‚Ð¸Ð¿Ñƒ"
=======
    "#### By Type"
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "096cd904-72f0-4ebe-a8b7-d0e730faea7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'event': 'on_chat_model_start', 'data': {'input': 'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`'}, 'name': 'model', 'tags': ['seq:step:1'], 'run_id': 'db246792-2a91-4eb3-a14b-29658947065d', 'metadata': {}}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='{', id='run-db246792-2a91-4eb3-a14b-29658947065d')}, 'run_id': 'db246792-2a91-4eb3-a14b-29658947065d', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {}}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='\\n  ', id='run-db246792-2a91-4eb3-a14b-29658947065d')}, 'run_id': 'db246792-2a91-4eb3-a14b-29658947065d', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {}}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='\"', id='run-db246792-2a91-4eb3-a14b-29658947065d')}, 'run_id': 'db246792-2a91-4eb3-a14b-29658947065d', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {}}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='countries', id='run-db246792-2a91-4eb3-a14b-29658947065d')}, 'run_id': 'db246792-2a91-4eb3-a14b-29658947065d', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {}}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='\":', id='run-db246792-2a91-4eb3-a14b-29658947065d')}, 'run_id': 'db246792-2a91-4eb3-a14b-29658947065d', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {}}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' [', id='run-db246792-2a91-4eb3-a14b-29658947065d')}, 'run_id': 'db246792-2a91-4eb3-a14b-29658947065d', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {}}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='\\n    ', id='run-db246792-2a91-4eb3-a14b-29658947065d')}, 'run_id': 'db246792-2a91-4eb3-a14b-29658947065d', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {}}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='{', id='run-db246792-2a91-4eb3-a14b-29658947065d')}, 'run_id': 'db246792-2a91-4eb3-a14b-29658947065d', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {}}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='\\n      ', id='run-db246792-2a91-4eb3-a14b-29658947065d')}, 'run_id': 'db246792-2a91-4eb3-a14b-29658947065d', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {}}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='\"', id='run-db246792-2a91-4eb3-a14b-29658947065d')}, 'run_id': 'db246792-2a91-4eb3-a14b-29658947065d', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {}}\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "chain = model.with_config({\"run_name\": \"model\"}) | JsonOutputParser().with_config(\n",
    "    {\"run_name\": \"my_parser\"}\n",
    ")\n",
    "\n",
    "max_events = 0\n",
    "async for event in chain.astream_events(\n",
    "    'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`',\n",
    "    version=\"v2\",\n",
    "    include_types=[\"chat_model\"],\n",
    "):\n",
    "    print(event)\n",
    "    max_events += 1\n",
    "    if max_events > 10:\n",
    "        # Truncate output\n",
    "        print(\"...\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ec8dd4-9b5b-4000-b63f-5845bfc5a065",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "#### ÐŸÐ¾ Ñ‚ÐµÐ³Ð°Ð¼\n",
    "\n",
    ":::caution\n",
    "\n",
    "Ð¢ÐµÐ³Ð¸ Ð½Ð°ÑÐ»ÐµÐ´ÑƒÑŽÑ‚ÑÑ Ð´Ð¾Ñ‡ÐµÑ€Ð½Ð¸Ð¼Ð¸ ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ð°Ð¼Ð¸, Ð¿Ð¾ÑÑ‚Ð¾Ð¼Ñƒ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿Ð¾ Ñ‚ÐµÐ³Ð°Ð¼ ÑÐ»ÐµÐ´ÑƒÐµÑ‚ Ñ Ð¾ÑÑ‚Ð¾Ñ€Ð¾Ð¶Ð½Ð¾ÑÑ‚ÑŒÑŽ.\n",
    "\n",
=======
    "#### By Tags\n",
    "\n",
    ":::{.callout-caution}\n",
    "\n",
    "Tags are inherited by child components of a given runnable. \n",
    "\n",
    "If you're using tags to filter, make sure that this is what you want.\n",
>>>>>>> langchan/master
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26bac0d2-76d9-446e-b346-82790236b88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'event': 'on_chain_start', 'data': {'input': 'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`'}, 'name': 'RunnableSequence', 'tags': ['my_chain'], 'run_id': 'fd68dd64-7a4d-4bdb-a0c2-ee592db0d024', 'metadata': {}}\n",
      "{'event': 'on_chat_model_start', 'data': {'input': {'messages': [[HumanMessage(content='output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`')]]}}, 'name': 'ChatAnthropic', 'tags': ['seq:step:1', 'my_chain'], 'run_id': 'efd3c8af-4be5-4f6c-9327-e3f9865dd1cd', 'metadata': {}}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='{', id='run-efd3c8af-4be5-4f6c-9327-e3f9865dd1cd')}, 'run_id': 'efd3c8af-4be5-4f6c-9327-e3f9865dd1cd', 'name': 'ChatAnthropic', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {}}\n",
      "{'event': 'on_parser_start', 'data': {}, 'name': 'JsonOutputParser', 'tags': ['seq:step:2', 'my_chain'], 'run_id': 'afde30b9-beac-4b36-b4c7-dbbe423ddcdb', 'metadata': {}}\n",
      "{'event': 'on_parser_stream', 'data': {'chunk': {}}, 'run_id': 'afde30b9-beac-4b36-b4c7-dbbe423ddcdb', 'name': 'JsonOutputParser', 'tags': ['seq:step:2', 'my_chain'], 'metadata': {}}\n",
      "{'event': 'on_chain_stream', 'data': {'chunk': {}}, 'run_id': 'fd68dd64-7a4d-4bdb-a0c2-ee592db0d024', 'name': 'RunnableSequence', 'tags': ['my_chain'], 'metadata': {}}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='\\n  ', id='run-efd3c8af-4be5-4f6c-9327-e3f9865dd1cd')}, 'run_id': 'efd3c8af-4be5-4f6c-9327-e3f9865dd1cd', 'name': 'ChatAnthropic', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {}}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='\"', id='run-efd3c8af-4be5-4f6c-9327-e3f9865dd1cd')}, 'run_id': 'efd3c8af-4be5-4f6c-9327-e3f9865dd1cd', 'name': 'ChatAnthropic', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {}}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='countries', id='run-efd3c8af-4be5-4f6c-9327-e3f9865dd1cd')}, 'run_id': 'efd3c8af-4be5-4f6c-9327-e3f9865dd1cd', 'name': 'ChatAnthropic', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {}}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='\":', id='run-efd3c8af-4be5-4f6c-9327-e3f9865dd1cd')}, 'run_id': 'efd3c8af-4be5-4f6c-9327-e3f9865dd1cd', 'name': 'ChatAnthropic', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {}}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' [', id='run-efd3c8af-4be5-4f6c-9327-e3f9865dd1cd')}, 'run_id': 'efd3c8af-4be5-4f6c-9327-e3f9865dd1cd', 'name': 'ChatAnthropic', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {}}\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "chain = (model | JsonOutputParser()).with_config({\"tags\": [\"my_chain\"]})\n",
    "\n",
    "max_events = 0\n",
    "async for event in chain.astream_events(\n",
    "    'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`',\n",
    "    version=\"v2\",\n",
    "    include_tags=[\"my_chain\"],\n",
    "):\n",
    "    print(event)\n",
    "    max_events += 1\n",
    "    if max_events > 10:\n",
    "        # Truncate output\n",
    "        print(\"...\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05e54c4-61a2-4f6c-aa68-d2b09b5e1d4f",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "### ÐšÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ñ‹ Ð±ÐµÐ· Ð¿Ð¾Ñ‚Ð¾ÐºÐ¾Ð²Ð¾Ð¹ Ð¿ÐµÑ€ÐµÐ´Ð°Ñ‡Ð¸\n",
    "\n",
    "ÐœÐµÑ‚Ð¾Ð´ `astream_events` Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÑ‚ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð¿Ð¾Ñ‚Ð¾Ðº ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ð¹ Ð¾Ñ‚ Ð¿Ñ€Ð¾Ð¼ÐµÐ¶ÑƒÑ‚Ð¾Ñ‡Ð½Ñ‹Ñ… ÑˆÐ°Ð³Ð¾Ð², Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÑŽÑ‰Ð¸Ñ… Ñ€Ð°Ð±Ð¾Ñ‚Ñƒ Ñ Ð¿Ð¾Ñ‚Ð¾ÐºÐ¾Ð¼ Ð´Ð°Ð½Ð½Ñ‹Ñ…."
=======
    "### Non-streaming components\n",
    "\n",
    "Remember how some components don't stream well because they don't operate on **input streams**?\n",
    "\n",
    "While such components can break streaming of the final output when using `astream`, `astream_events` will still yield streaming events from intermediate steps that support streaming!"
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e6451d3-3b11-4a71-ae19-998f4c10180f",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "# Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ð½Ðµ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÐµÑ‚ Ñ€Ð°Ð±Ð¾Ñ‚Ñƒ Ñ Ð¿Ð¾Ñ‚Ð¾ÐºÐ¾Ð¼ Ð´Ð°Ð½Ð½Ñ‹Ñ….\n",
=======
    "# Function that does not support streaming.\n",
    "# It operates on the finalizes inputs rather than\n",
    "# operating on the input stream.\n",
>>>>>>> langchan/master
    "def _extract_country_names(inputs):\n",
    "    \"\"\"A function that does not operates on input streams and breaks streaming.\"\"\"\n",
    "    if not isinstance(inputs, dict):\n",
    "        return \"\"\n",
    "\n",
    "    if \"countries\" not in inputs:\n",
    "        return \"\"\n",
    "\n",
    "    countries = inputs[\"countries\"]\n",
    "\n",
    "    if not isinstance(countries, list):\n",
    "        return \"\"\n",
    "\n",
    "    country_names = [\n",
    "        country.get(\"name\") for country in countries if isinstance(country, dict)\n",
    "    ]\n",
    "    return country_names\n",
    "\n",
    "\n",
    "chain = (\n",
    "    model | JsonOutputParser() | _extract_country_names\n",
    ")  # This parser only works with OpenAI right now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a972e1a6-80cd-4d59-90a0-73563f1503d4",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "ÐœÐµÑ‚Ð¾Ð´ `astream` API Ð½Ðµ Ð±ÑƒÐ´ÐµÑ‚ Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ð¾."
=======
    "As expected, the `astream` API doesn't work correctly because `_extract_country_names` doesn't operate on streams."
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9a8fe35-faab-4970-b8c0-5c780845d98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['France', 'Spain', 'Japan']\n"
     ]
    }
   ],
   "source": [
    "async for chunk in chain.astream(\n",
    "    \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n",
    "    'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n",
    "    \"Each country should have the key `name` and `population`\",\n",
    "):\n",
    "    print(chunk, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b279ea33-54f1-400a-acb1-b8445ccbf1fa",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "Ð£Ð±ÐµÐ´Ð¸Ð¼ÑÑ, Ñ‡Ñ‚Ð¾ Ð¼ÐµÑ‚Ð¾Ð´ `astream_events` Ð¿Ð¾-Ð¿Ñ€ÐµÐ¶Ð½ÐµÐ¼Ñƒ Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð´Ð¾ÑÑ‚ÑƒÐ¿ Ðº Ð¿Ð¾Ñ‚Ð¾ÐºÑƒ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¾Ñ‚ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¸ Ð¿Ð°Ñ€ÑÐµÑ€Ð°."
=======
    "Now, let's confirm that with astream_events we're still seeing streaming output from the model and the parser."
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b08215cd-bffa-4e76-aaf3-c52ee34f152c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat model chunk: '{'\n",
      "Parser chunk: {}\n",
      "Chat model chunk: '\\n  '\n",
      "Chat model chunk: '\"'\n",
      "Chat model chunk: 'countries'\n",
      "Chat model chunk: '\":'\n",
      "Chat model chunk: ' ['\n",
      "Parser chunk: {'countries': []}\n",
      "Chat model chunk: '\\n    '\n",
      "Chat model chunk: '{'\n",
      "Parser chunk: {'countries': [{}]}\n",
      "Chat model chunk: '\\n      '\n",
      "Chat model chunk: '\"'\n",
      "Chat model chunk: 'name'\n",
      "Chat model chunk: '\":'\n",
      "Chat model chunk: ' \"'\n",
      "Parser chunk: {'countries': [{'name': ''}]}\n",
      "Chat model chunk: 'France'\n",
      "Parser chunk: {'countries': [{'name': 'France'}]}\n",
      "Chat model chunk: '\",'\n",
      "Chat model chunk: '\\n      '\n",
      "Chat model chunk: '\"'\n",
      "Chat model chunk: 'population'\n",
      "Chat model chunk: '\":'\n",
      "Chat model chunk: ' '\n",
      "Chat model chunk: '67'\n",
      "Parser chunk: {'countries': [{'name': 'France', 'population': 67}]}\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "num_events = 0\n",
    "\n",
    "async for event in chain.astream_events(\n",
    "    \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n",
    "    'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n",
    "    \"Each country should have the key `name` and `population`\",\n",
    "    version=\"v2\",\n",
    "):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        print(\n",
    "            f\"Chat model chunk: {repr(event['data']['chunk'].content)}\",\n",
    "            flush=True,\n",
    "        )\n",
    "    if kind == \"on_parser_stream\":\n",
    "        print(f\"Parser chunk: {event['data']['chunk']}\", flush=True)\n",
    "    num_events += 1\n",
    "    if num_events > 30:\n",
    "        # Truncate the output\n",
    "        print(\"...\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e91bdd3-f4a3-4b3c-b21a-26365c6c1566",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "### Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ñ‹Ñ… Ð²Ñ‹Ð·Ð¾Ð²Ð¾Ð²\n",
    "\n",
    ":::caution\n",
    "\n",
    "ÐŸÑ€Ð¸ Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ð¸ Ð² ÑÐ²Ð¾Ð¸ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ñ‹ Ð²Ñ‹Ð·Ñ‹Ð²Ð°ÐµÐ¼Ñ‹Ñ… (`invoke`) Runnable, Ð²Ð°Ð¼ Ð½ÑƒÐ¶Ð½Ð¾ Ð¿Ñ€ÐµÐ´ÑƒÑÐ¼Ð¾Ñ‚Ñ€ÐµÑ‚ÑŒ Ð½Ð°Ð»Ð¸Ñ‡Ð¸Ðµ Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ñ‹Ñ… Ð²Ñ‹Ð·Ð¾Ð²Ð¾Ð² Ð´Ð»Ñ Runnable.\n",
    "Ð’ Ð¿Ñ€Ð¾Ñ‚Ð¸Ð²Ð½Ð¾Ð¼ ÑÐ»ÑƒÑ‡Ð°Ðµ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ñ Ð½Ðµ Ð±ÑƒÐ´ÑƒÑ‚ Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒÑÑ.\n",
    "\n",
    ":::\n",
    "\n",
    ":::note\n",
    "\n",
    "ÐŸÑ€Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ð¸ RunnableLambdas Ð¸Ð»Ð¸ Ð´ÐµÐºÐ¾Ñ€Ð°Ñ‚Ð¾Ñ€Ð° `@chain` Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ñ‹Ðµ Ð²Ñ‹Ð·Ð¾Ð²Ñ‹ Ð´Ð¾Ð±Ð°Ð²Ð»ÑÑŽÑ‚ÑÑ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸.\n",
    "\n",
=======
    "### Propagating Callbacks\n",
    "\n",
    ":::{.callout-caution}\n",
    "If you're using invoking runnables inside your tools, you need to propagate callbacks to the runnable; otherwise, no stream events will be generated.\n",
    ":::\n",
    "\n",
    ":::{.callout-note}\n",
    "When using `RunnableLambdas` or `@chain` decorator, callbacks are propagated automatically behind the scenes.\n",
>>>>>>> langchan/master
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1854206d-b3a5-4f91-9e00-bccbaebac61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'event': 'on_tool_start', 'data': {'input': 'hello'}, 'name': 'bad_tool', 'tags': [], 'run_id': 'ea900472-a8f7-425d-b627-facdef936ee8', 'metadata': {}}\n",
      "{'event': 'on_chain_start', 'data': {'input': 'hello'}, 'name': 'reverse_word', 'tags': [], 'run_id': '77b01284-0515-48f4-8d7c-eb27c1882f86', 'metadata': {}}\n",
      "{'event': 'on_chain_end', 'data': {'output': 'olleh', 'input': 'hello'}, 'run_id': '77b01284-0515-48f4-8d7c-eb27c1882f86', 'name': 'reverse_word', 'tags': [], 'metadata': {}}\n",
      "{'event': 'on_tool_end', 'data': {'output': 'olleh'}, 'run_id': 'ea900472-a8f7-425d-b627-facdef936ee8', 'name': 'bad_tool', 'tags': [], 'metadata': {}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "def reverse_word(word: str):\n",
    "    return word[::-1]\n",
    "\n",
    "\n",
    "reverse_word = RunnableLambda(reverse_word)\n",
    "\n",
    "\n",
    "@tool\n",
    "def bad_tool(word: str):\n",
    "    \"\"\"Custom tool that doesn't propagate callbacks.\"\"\"\n",
    "    return reverse_word.invoke(word)\n",
    "\n",
    "\n",
    "async for event in bad_tool.astream_events(\"hello\", version=\"v2\"):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e68a99-7886-465b-8575-116022857469",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "ÐÐ¸Ð¶Ðµ Ð¿Ñ€Ð¸Ð¼ÐµÑ€ Ñ‚Ð¾Ð³Ð¾ ÐºÐ°Ðº Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¾ Ð´Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ñ‹Ðµ Ð²Ñ‹Ð·Ð¾Ð²Ñ‹.\n",
    "ÐŸÑ€Ð¸ Ð·Ð°Ð¿ÑƒÑÐºÐµ Ð²Ñ‹ Ð´Ð¾Ð»Ð¶Ð½Ñ‹ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ñ Ð¾Ñ‚ Runnable `reverse_word`."
=======
    "Here's a re-implementation that does propagate callbacks correctly. You'll notice that now we're getting events from the `reverse_word` runnable as well."
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a20a6cb3-bb43-465c-8cfc-0a7349d70968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'event': 'on_tool_start', 'data': {'input': 'hello'}, 'name': 'correct_tool', 'tags': [], 'run_id': 'd5ea83b9-9278-49cc-9f1d-aa302d671040', 'metadata': {}}\n",
      "{'event': 'on_chain_start', 'data': {'input': 'hello'}, 'name': 'reverse_word', 'tags': [], 'run_id': '44dafbf4-2f87-412b-ae0e-9f71713810df', 'metadata': {}}\n",
      "{'event': 'on_chain_end', 'data': {'output': 'olleh', 'input': 'hello'}, 'run_id': '44dafbf4-2f87-412b-ae0e-9f71713810df', 'name': 'reverse_word', 'tags': [], 'metadata': {}}\n",
      "{'event': 'on_tool_end', 'data': {'output': 'olleh'}, 'run_id': 'd5ea83b9-9278-49cc-9f1d-aa302d671040', 'name': 'correct_tool', 'tags': [], 'metadata': {}}\n"
     ]
    }
   ],
   "source": [
    "@tool\n",
    "def correct_tool(word: str, callbacks):\n",
    "    \"\"\"A tool that correctly propagates callbacks.\"\"\"\n",
    "    return reverse_word.invoke(word, {\"callbacks\": callbacks})\n",
    "\n",
    "\n",
    "async for event in correct_tool.astream_events(\"hello\", version=\"v2\"):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640daa94-e4fe-4997-ab6e-45120f18b9ee",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "ÐŸÑ€Ð¸Ð¼ÐµÑ€ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ñ‡Ð¸ÐµÑÐºÐ¾Ð³Ð¾ Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ñ‹Ñ… Ð²Ñ‹Ð·Ð¾Ð²Ð¾Ð² Ð¿Ñ€Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ð¸ Runnable Lambdas."
=======
    "If you're invoking runnables from within Runnable Lambdas or `@chains`, then callbacks will be passed automatically on your behalf."
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ac0a3c1-f3a4-4157-b053-4fec8d2e698c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'event': 'on_chain_start', 'data': {'input': '1234'}, 'name': 'reverse_and_double', 'tags': [], 'run_id': '03b0e6a1-3e60-42fc-8373-1e7829198d80', 'metadata': {}}\n",
      "{'event': 'on_chain_start', 'data': {'input': '1234'}, 'name': 'reverse_word', 'tags': [], 'run_id': '5cf26fc8-840b-4642-98ed-623dda28707a', 'metadata': {}}\n",
      "{'event': 'on_chain_end', 'data': {'output': '4321', 'input': '1234'}, 'run_id': '5cf26fc8-840b-4642-98ed-623dda28707a', 'name': 'reverse_word', 'tags': [], 'metadata': {}}\n",
      "{'event': 'on_chain_stream', 'data': {'chunk': '43214321'}, 'run_id': '03b0e6a1-3e60-42fc-8373-1e7829198d80', 'name': 'reverse_and_double', 'tags': [], 'metadata': {}}\n",
      "{'event': 'on_chain_end', 'data': {'output': '43214321'}, 'run_id': '03b0e6a1-3e60-42fc-8373-1e7829198d80', 'name': 'reverse_and_double', 'tags': [], 'metadata': {}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "\n",
    "async def reverse_and_double(word: str):\n",
    "    return await reverse_word.ainvoke(word) * 2\n",
    "\n",
    "\n",
    "reverse_and_double = RunnableLambda(reverse_and_double)\n",
    "\n",
    "await reverse_and_double.ainvoke(\"1234\")\n",
    "\n",
    "async for event in reverse_and_double.astream_events(\"1234\", version=\"v2\"):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a34268-9b3d-4857-b4ed-65d95f4a1293",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "ÐŸÑ€Ð¸Ð¼ÐµÑ€ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ñ‡Ð¸ÐµÑÐºÐ¾Ð³Ð¾ Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ñ‹Ñ… Ð²Ñ‹Ð·Ð¾Ð²Ð¾Ð² Ð¿Ñ€Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ð¸ Ð´ÐµÐºÐ¾Ñ€Ð°Ñ‚Ð¾Ñ€Ð° `@chain`."
=======
    "And with the `@chain` decorator:"
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c896bb94-9d10-41ff-8fe2-d6b05b1ed74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'event': 'on_chain_start', 'data': {'input': '1234'}, 'name': 'reverse_and_double', 'tags': [], 'run_id': '1bfcaedc-f4aa-4d8e-beee-9bba6ef17008', 'metadata': {}}\n",
      "{'event': 'on_chain_start', 'data': {'input': '1234'}, 'name': 'reverse_word', 'tags': [], 'run_id': '64fc99f0-5d7d-442b-b4f5-4537129f67d1', 'metadata': {}}\n",
      "{'event': 'on_chain_end', 'data': {'output': '4321', 'input': '1234'}, 'run_id': '64fc99f0-5d7d-442b-b4f5-4537129f67d1', 'name': 'reverse_word', 'tags': [], 'metadata': {}}\n",
      "{'event': 'on_chain_stream', 'data': {'chunk': '43214321'}, 'run_id': '1bfcaedc-f4aa-4d8e-beee-9bba6ef17008', 'name': 'reverse_and_double', 'tags': [], 'metadata': {}}\n",
      "{'event': 'on_chain_end', 'data': {'output': '43214321'}, 'run_id': '1bfcaedc-f4aa-4d8e-beee-9bba6ef17008', 'name': 'reverse_and_double', 'tags': [], 'metadata': {}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import chain\n",
    "\n",
    "\n",
    "@chain\n",
    "async def reverse_and_double(word: str):\n",
    "    return await reverse_word.ainvoke(word) * 2\n",
    "\n",
    "\n",
    "await reverse_and_double.ainvoke(\"1234\")\n",
    "\n",
    "async for event in reverse_and_double.astream_events(\"1234\", version=\"v2\"):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3efcd9",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "Now you've learned some ways to stream both final outputs and internal steps with LangChain.\n",
    "\n",
    "To learn more, check out the other how-to guides in this section, or the [conceptual guide on Langchain Expression Language](/docs/concepts/#langchain-expression-language/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.-1.-1"
=======
   "version": "3.9.1"
>>>>>>> langchan/master
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
