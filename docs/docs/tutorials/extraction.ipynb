{
 "cells": [
  {
<<<<<<< HEAD
=======
   "cell_type": "raw",
   "id": "df29b30a-fd27-4e08-8269-870df5631f9e",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_position: 4\n",
    "---"
   ]
  },
  {
>>>>>>> langchan/master
   "cell_type": "markdown",
   "id": "d28530a6-ddfd-49c0-85dc-b723551f6614",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "# Ð‘Ñ‹ÑÑ‚Ñ€Ñ‹Ð¹ ÑÑ‚Ð°Ñ€Ñ‚\n",
    "\n",
    "Ð’ ÑÑ‚Ð¾Ð¼ Ñ€Ð°Ð·Ð´ÐµÐ»Ðµ Ð²Ñ‹ ÑƒÐ·Ð½Ð°ÐµÑ‚Ðµ ÐºÐ°Ðº Ð¼Ð¾Ð¶Ð½Ð¾ Ð¸Ð·Ð²Ð»ÐµÐºÐ°Ñ‚ÑŒ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½ÑƒÑŽ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ GigaChat Ð¸ Ð²Ñ‹Ð·Ð¾Ð²Ð° Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²/Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¹."
=======
    "# Build an Extraction Chain\n",
    "\n",
    "In this tutorial, we will build a chain to extract structured information from unstructured text. \n",
    "\n",
    ":::{.callout-important}\n",
    "This tutorial will only work with models that support **function/tool calling**\n",
    ":::\n",
    "\n",
    "## Concepts\n",
    "\n",
    "Concepts we will cover are:\n",
    "- Using [language models](/docs/concepts/#chat-models)\n",
    "- Using [function/tool calling](/docs/concepts/#function-tool-calling)\n",
    "- Debugging and tracing your application using [LangSmith](/docs/concepts/#langsmith)\n"
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4412def2-38e3-4bd0-bbf0-fb09ff9e5985",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ°\n",
    "\n",
    "Ð”Ð»Ñ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ñ [ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…](/docs/modules/model_io/chat/structured_output) Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ðµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑŽÑ‚ÑÑ [Ð¼Ð¾Ð´ÐµÐ»Ð¸ GigaChat Ñ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ¾Ð¹ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¹](https://developers.sber.ru/docs/ru/gigachat/models#modeli-dlya-generatsii)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380c0425-6062-4837-8630-c220240c83b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gigachain"
=======
    "## Setup\n",
    "\n",
    "### Jupyter Notebook\n",
    "\n",
    "This guide (and most of the other guides in the documentation) uses [Jupyter notebooks](https://jupyter.org/) and assumes the reader is as well. Jupyter notebooks are perfect for learning how to work with LLM systems because oftentimes things can go wrong (unexpected output, API down, etc) and going through guides in an interactive environment is a great way to better understand them.\n",
    "\n",
    "This and other tutorials are perhaps most conveniently run in a Jupyter notebook. See [here](https://jupyter.org/install) for instructions on how to install.\n",
    "\n",
    "### Installation\n",
    "\n",
    "To install LangChain run:\n",
    "\n",
    "```{=mdx}\n",
    "import Tabs from '@theme/Tabs';\n",
    "import TabItem from '@theme/TabItem';\n",
    "import CodeBlock from \"@theme/CodeBlock\";\n",
    "\n",
    "<Tabs>\n",
    "  <TabItem value=\"pip\" label=\"Pip\" default>\n",
    "    <CodeBlock language=\"bash\">pip install langchain</CodeBlock>\n",
    "  </TabItem>\n",
    "  <TabItem value=\"conda\" label=\"Conda\">\n",
    "    <CodeBlock language=\"bash\">conda install langchain -c conda-forge</CodeBlock>\n",
    "  </TabItem>\n",
    "</Tabs>\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "For more details, see our [Installation guide](/docs/how_to/installation).\n",
    "\n",
    "### LangSmith\n",
    "\n",
    "Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\n",
    "As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\n",
    "The best way to do this is with [LangSmith](https://smith.langchain.com).\n",
    "\n",
    "After you sign up at the link above, make sure to set your environment variables to start logging traces:\n",
    "\n",
    "```shell\n",
    "export LANGCHAIN_TRACING_V2=\"true\"\n",
    "export LANGCHAIN_API_KEY=\"...\"\n",
    "```\n",
    "\n",
    "Or, if in a notebook, you can set them with:\n",
    "\n",
    "```python\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()\n",
    "```"
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d6b970-2ea3-4192-951e-21237212b359",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## Ð¡Ñ…ÐµÐ¼Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ…\n",
    "\n",
    "Ð¡Ð½Ð°Ñ‡Ð°Ð»Ð° Ð½ÑƒÐ¶Ð½Ð¾ Ð¾Ð¿Ð¸ÑÐ°Ñ‚ÑŒ ÐºÐ°ÐºÑƒÑŽ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ÑÑ Ð¸Ð·Ð²Ð»ÐµÑ‡ÑŒ Ð¸Ð· Ñ‚ÐµÐºÑÑ‚Ð°.\n",
    "\n",
    "Ð”Ð»Ñ Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ñ Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð° ÑÑ…ÐµÐ¼Ñ‹ Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ Pydantic."
=======
    "## The Schema\n",
    "\n",
    "First, we need to describe what information we want to extract from the text.\n",
    "\n",
    "We'll use Pydantic to define an example schema  to extract personal information."
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 16,
=======
   "execution_count": 5,
>>>>>>> langchan/master
   "id": "c141084c-fb94-4093-8d6a-81175d688e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class Person(BaseModel):\n",
<<<<<<< HEAD
    "    \"\"\"Ð˜Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ Ð¾ Ñ‡ÐµÐ»Ð¾Ð²ÐµÐºÐµ.\"\"\"\n",
    "\n",
    "    # Ð”Ð¾Ðº-ÑÑ‚Ñ€Ð¾ÐºÐ° Ð²Ñ‹ÑˆÐµ, Ð¿ÐµÑ€ÐµÐ´Ð°ÐµÑ‚ÑÑ Ð² Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ð¸ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸\n",
    "    # Ð¸ Ð¿Ð¾Ð¼Ð¾Ð³Ð°ÐµÑ‚ ÑƒÐ»ÑƒÑ‡ÑˆÐ¸Ñ‚ÑŒ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ LLM\n",
    "\n",
    "    # ÐžÐ±Ñ€Ð°Ñ‚Ð¸Ñ‚Ðµ Ð²Ð½Ð¸Ð¼Ð°Ð½Ð¸Ðµ:\n",
    "    # * Ð’ÑÐµ Ð¿Ð¾Ð»Ñ â€” Ð½ÐµÐ¾Ð±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ (`optional`). Ð­Ñ‚Ð¾ Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÑ‚ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð½Ðµ Ð¸Ð·Ð²Ð»ÐµÐºÐ°Ñ‚ÑŒ Ð½ÐµÐ¾Ð¿Ð¸ÑÐ°Ð½Ð½Ñ‹Ðµ Ð¿Ð¾Ð»Ñ.\n",
    "    # * Ð£ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ð¿Ð¾Ð»Ñ ÐµÑÑ‚ÑŒ Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ðµ (`description`), ÐºÐ¾Ñ‚Ð¾Ñ€Ð¾Ðµ Ð¿ÐµÑ€ÐµÐ´Ð°ÐµÑ‚ÑÑ Ð² Ð¼Ð¾Ð´ÐµÐ»ÑŒ, Ð² Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ð¸ Ð°Ñ€Ð³ÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð² Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸.\n",
    "    # Ð¥Ð¾Ñ€Ð¾ÑˆÐµÐµ Ð¿Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ðµ Ð¿Ð¾Ð¼Ð¾Ð³Ð°ÐµÑ‚ Ð¿Ð¾Ð²Ñ‹ÑÐ¸Ñ‚ÑŒ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ñ.\n",
    "    name: Optional[str] = Field(..., description=\"Ð˜Ð¼Ñ Ñ‡ÐµÐ»Ð¾Ð²ÐµÐºÐ°\")\n",
    "    hair_color: Optional[str] = Field(\n",
    "        ..., description=\"Ð¦Ð²ÐµÑ‚ Ð²Ð¾Ð»Ð¾Ñ Ñ‡ÐµÐ»Ð¾Ð²ÐµÐºÐ°, Ð·Ð°Ð¿Ð¾Ð»Ð½Ð¸ ÐµÑÐ»Ð¸ Ð¸Ð·Ð²ÐµÑÑ‚ÐµÐ½\"\n",
    "    )\n",
    "    height_in_meters: Optional[float] = Field(\n",
    "        ..., description=\"Ð’Ñ‹ÑÐ¾Ñ‚Ð° Ñ‡ÐµÐ»Ð¾Ð²ÐµÐºÐ° Ð² Ð¼ÐµÑ‚Ñ€Ð°Ñ….\"\n",
=======
    "    \"\"\"Information about a person.\"\"\"\n",
    "\n",
    "    # ^ Doc-string for the entity Person.\n",
    "    # This doc-string is sent to the LLM as the description of the schema Person,\n",
    "    # and it can help to improve extraction results.\n",
    "\n",
    "    # Note that:\n",
    "    # 1. Each field is an `optional` -- this allows the model to decline to extract it!\n",
    "    # 2. Each field has a `description` -- this description is used by the LLM.\n",
    "    # Having a good description can help improve extraction results.\n",
    "    name: Optional[str] = Field(default=None, description=\"The name of the person\")\n",
    "    hair_color: Optional[str] = Field(\n",
    "        default=None, description=\"The color of the peron's hair if known\"\n",
    "    )\n",
    "    height_in_meters: Optional[str] = Field(\n",
    "        default=None, description=\"Height measured in meters\"\n",
>>>>>>> langchan/master
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f248dd54-e36d-435a-b154-394ab4ed6792",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "ÐŸÑ€Ð¸ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ð¸ ÑÑ…ÐµÐ¼ Ð¿Ñ€Ð¸Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°Ð¹Ñ‚ÐµÑÑŒ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ñ… Ð¿Ñ€Ð°Ð²Ð¸Ð»:\n",
    "\n",
    "* ÐžÐ¿Ð¸ÑÑ‹Ð²Ð°Ð¹Ñ‚Ðµ Ð°Ñ‚Ñ€Ð¸Ð±ÑƒÑ‚Ñ‹ Ð¸ ÑÐ°Ð¼Ñƒ ÑÑ…ÐµÐ¼Ñƒ. ÐžÐ¿Ð¸ÑÐ°Ð½Ð¸Ñ Ð¿ÐµÑ€ÐµÐ´Ð°ÑŽÑ‚ÑÑ Ð² Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑŽÑ‚ÑÑ Ð´Ð»Ñ Ð¿Ð¾Ð²Ñ‹ÑˆÐµÐ½Ð¸Ñ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð° Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ñ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸.\n",
    "* ÐÐµ Ð²Ñ‹Ð½ÑƒÐ¶Ð´Ð°Ð¹Ñ‚Ðµ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð¿Ñ€Ð¸Ð´ÑƒÐ¼Ñ‹Ð²Ð°Ñ‚ÑŒ Ð´Ð°Ð½Ð½Ñ‹Ðµ. Ð’ Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ðµ Ð²Ñ‹ÑˆÐµ Ð°Ñ‚Ñ€Ð¸Ð±ÑƒÑ‚Ñ‹ Ð¾Ñ‚Ð¼ÐµÑ‡ÐµÐ½Ñ‹ ÐºÐ°Ðº Ð½ÐµÐ¾Ð±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ (`Optional`), Ñ‡Ñ‚Ð¾ Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÑ‚ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°Ñ‚ÑŒ `None`, ÐµÑÐ»Ð¸ Ð¾Ð½Ð° Ð½Ðµ Ð·Ð½Ð°ÐµÑ‚ Ð¾Ñ‚Ð²ÐµÑ‚Ð°.\n",
    "\n",
    "## Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ ÑÐºÑÑ‚Ñ€Ð°ÐºÑ‚Ð¾Ñ€Ð°\n",
    "\n",
    "ÐŸÑ€Ð¸Ð¼ÐµÑ€ Ð½Ð¸Ð¶Ðµ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð¸Ñ€ÑƒÐµÑ‚ ÐºÐ¾Ð´ ÑÐºÑÑ‚Ñ€Ð°ÐºÑ‚Ð¾Ñ€Ð° Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð·Ð°Ð´Ð°Ð½Ð½Ð¾Ð¹ ÑÑ…ÐµÐ¼Ñ‹."
=======
    "There are two best practices when defining schema:\n",
    "\n",
    "1. Document the **attributes** and the **schema** itself: This information is sent to the LLM and is used to improve the quality of information extraction.\n",
    "2. Do not force the LLM to make up information! Above we used `Optional` for the attributes allowing the LLM to output `None` if it doesn't know the answer.\n",
    "\n",
    ":::{.callout-important}\n",
    "For best performance, document the schema well and make sure the model isn't force to return results if there's no information to be extracted in the text.\n",
    ":::\n",
    "\n",
    "## The Extractor\n",
    "\n",
    "Let's create an information extractor using the schema we defined above."
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 17,
=======
   "execution_count": 6,
>>>>>>> langchan/master
   "id": "a5e490f6-35ad-455e-8ae4-2bae021583ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
<<<<<<< HEAD
    "# ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚: Ð´Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ð¸ Ð¸ Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ð¹ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚\n",
    "# ÐÐ° ÑÑ‚Ð¾Ð¼ ÑÑ‚Ð°Ð¿Ðµ Ð¼Ð¾Ð¶Ð½Ð¾:\n",
    "# * Ð”Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ñ‹ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¹, Ð´Ð»Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð° Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ñ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸\n",
    "# * ÐŸÑ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð¸Ñ‚ÑŒ Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½ÑƒÑŽ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¾ Ñ‚Ð¾Ð¼ ÐºÐ°ÐºÐ¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¸ Ð¾Ñ‚ÐºÑƒÐ´Ð° Ð±ÑƒÐ´ÑƒÑ‚ Ð¸Ð·Ð²Ð»ÐµÐºÐ°Ñ‚ÑŒÑÑ\n",
=======
    "# Define a custom prompt to provide instructions and any additional context.\n",
    "# 1) You can add examples into the prompt template to improve extraction quality\n",
    "# 2) Introduce additional parameters to take context into account (e.g., include metadata\n",
    "#    about the document from which the text was extracted.)\n",
>>>>>>> langchan/master
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
<<<<<<< HEAD
    "            \"Ð¢Ñ‹ ÑÐºÑÐ¿ÐµÑ€Ñ‚ Ð² Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ð¸ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸ Ð¸Ð· Ñ‚ÐµÐºÑÑ‚Ð°. \"\n",
    "            \"Ð˜Ð·Ð²Ð»ÐµÐºÐ°Ð¹ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½ÑƒÑŽ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¸Ð· Ñ‚ÐµÐºÑÑ‚Ð°. \"\n",
    "            \"Ð•ÑÐ»Ð¸ Ñ‚Ñ‹ Ð½Ðµ Ð·Ð½Ð°ÐµÑˆÑŒ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ Ð°Ñ‚Ñ‚Ñ€Ð¸Ð±ÑƒÑ‚Ð°, \"\n",
    "            \"ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð½ÑƒÐ¶Ð½Ð¾ Ð¸Ð·Ð²Ð»ÐµÑ‡ÑŒ, Ð¿Ð¾ÑÑ‚Ð°Ð²ÑŒ Ð°Ñ‚Ñ‚Ñ€Ð¸Ð±ÑƒÑ‚Ñƒ null.\",\n",
    "        ),\n",
    "        # Ðž Ñ‚Ð¾Ð¼ ÐºÐ°Ðº Ð¿Ð¾Ð²Ñ‹ÑÐ¸Ñ‚ÑŒ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð¾ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð¾Ð²\n",
    "        # Ñ‡Ð¸Ñ‚Ð°Ð¹Ñ‚Ðµ Ð² Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸ÑÑ…\n",
=======
    "            \"You are an expert extraction algorithm. \"\n",
    "            \"Only extract relevant information from the text. \"\n",
    "            \"If you do not know the value of an attribute asked to extract, \"\n",
    "            \"return null for the attribute's value.\",\n",
    "        ),\n",
    "        # Please see the how-to about improving performance with\n",
    "        # reference examples.\n",
>>>>>>> langchan/master
    "        # MessagesPlaceholder('examples'),\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832bf6a1-8e0c-4b6a-aa37-12fe9c42a6d9",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "Ð”Ð»Ñ ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ð¾Ð¹ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ð¿Ð¾Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÑŒ GigaChat Ñ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ¾Ð¹ Ð²Ñ‹Ð·Ð¾Ð²Ð° Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÑ‚Ð¾Ð²/Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¹."
=======
    "We need to use a model that supports function/tool calling.\n",
    "\n",
    "Please review [the documentation](/docs/concepts#function-tool-calling) for list of some models that can be used with this API."
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 18,
=======
   "execution_count": 7,
>>>>>>> langchan/master
   "id": "04d846a6-d5cb-4009-ac19-61e3aac0177e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonchase/workplace/langchain/libs/core/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: The method `ChatMistralAI.with_structured_output` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "from langchain_community.chat_models.gigachat import GigaChat\n",
    "\n",
    "llm = GigaChat(\n",
    "    timeout=6000,\n",
    "    model=\"GigaChat-Pro\",\n",
    "    temperature=0.01,\n",
    ")\n",
=======
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "llm = ChatMistralAI(model=\"mistral-large-latest\", temperature=0)\n",
>>>>>>> langchan/master
    "\n",
    "runnable = prompt | llm.with_structured_output(schema=Person)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23582c0b-00ed-403f-a10e-3aeabf921f12",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "Ð’Ñ‹Ð·Ð¾Ð² Ñ†ÐµÐ¿Ð¾Ñ‡ÐºÐ¸."
=======
    "Let's test it out"
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 19,
=======
   "execution_count": 8,
>>>>>>> langchan/master
   "id": "13165ac8-a1dc-44ce-a6ed-f52b577473e4",
   "metadata": {},
   "outputs": [
    {
<<<<<<< HEAD
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Giga generation stopped with reason: function_call\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Person(name='ÐÐ»Ð°Ð½ Ð¡Ð¼Ð¸Ñ‚', hair_color='Ð±Ð»Ð¾Ð½Ð´Ð¸Ð½', height_in_meters=1.85)"
      ]
     },
     "execution_count": 19,
=======
     "data": {
      "text/plain": [
       "Person(name='Alan Smith', hair_color='blond', height_in_meters='1.83')"
      ]
     },
     "execution_count": 8,
>>>>>>> langchan/master
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "text = \"ÐÐ»Ð°Ð½ Ð¡Ð¼Ð¸Ñ‚ Ð±Ð»Ð¾Ð½Ð´Ð¸Ð½, 1.85 Ð¼ÐµÑ‚Ñ€Ð° Ð²Ñ‹ÑÐ¾Ñ‚Ð¾Ð¹\"\n",
=======
    "text = \"Alan Smith is 6 feet tall and has blond hair.\"\n",
>>>>>>> langchan/master
    "runnable.invoke({\"text\": text})"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "28c5ef0c-b8d1-4e12-bd0e-e2528de87fcc",
   "metadata": {},
   "source": [
    "## Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¸Ñ… ÑÑƒÑ‰Ð½Ð¾ÑÑ‚ÐµÐ¹\n",
    "\n",
    "Ð’ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ð½ÑÑ‚Ð²Ðµ Ð·Ð°Ð´Ð°Ñ‡Ð° Ð²Ð°Ð¼ Ð¿Ð¾Ð½Ð°Ð´Ð¾Ð±Ð¸Ñ‚ÑÑ Ð¸Ð·Ð²Ð»ÐµÐºÐ°Ñ‚ÑŒ Ð¸Ð· Ñ‚ÐµÐºÑÑ‚Ð° Ð½Ðµ Ð¾Ð´Ð½Ñƒ ÑÑƒÑ‰ÑÐ½Ð¾Ñ‚ÑŒ, Ð° ÑÐ¿Ð¸ÑÐ¾Ðº.\n",
    "\n",
    "Ð”Ð»Ñ ÑÑ‚Ð¾Ð³Ð¾ Ð²Ñ‹ Ð¼Ð¾Ð¶ÐµÑ‚Ðµ ÑÐ¾Ð·Ð´Ð°Ð²Ð°Ñ‚ÑŒ Ð²Ð»Ð¾Ð¶ÐµÐ½Ð½Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ pydantic."
=======
   "id": "bd1c493d-f9dc-4236-8da9-50f6919f5710",
   "metadata": {},
   "source": [
    ":::{.callout-important} \n",
    "\n",
    "Extraction is Generative ðŸ¤¯\n",
    "\n",
    "LLMs are generative models, so they can do some pretty cool things like correctly extract the height of the person in meters\n",
    "even though it was provided in feet!\n",
    ":::\n",
    "\n",
    "We can see the LangSmith trace here: https://smith.langchain.com/public/44b69a63-3b3b-47b8-8a6d-61b46533f015/r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c5ef0c-b8d1-4e12-bd0e-e2528de87fcc",
   "metadata": {},
   "source": [
    "## Multiple Entities\n",
    "\n",
    "In **most cases**, you should be extracting a list of entities rather than a single entity.\n",
    "\n",
    "This can be easily achieved using pydantic by nesting models inside one another."
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 20,
=======
   "execution_count": 9,
>>>>>>> langchan/master
   "id": "591a0c16-7a17-4883-91ee-0d6d2fdb265c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class Person(BaseModel):\n",
<<<<<<< HEAD
    "    \"\"\"Ð˜Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ Ð¾ Ñ‡ÐµÐ»Ð¾Ð²ÐµÐºÐµ.\"\"\"\n",
    "\n",
    "    # Ð”Ð¾Ðº-ÑÑ‚Ñ€Ð¾ÐºÐ° Ð²Ñ‹ÑˆÐµ, Ð¿ÐµÑ€ÐµÐ´Ð°ÐµÑ‚ÑÑ Ð² Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ð¸ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸\n",
    "    # Ð¸ Ð¿Ð¾Ð¼Ð¾Ð³Ð°ÐµÑ‚ ÑƒÐ»ÑƒÑ‡ÑˆÐ¸Ñ‚ÑŒ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ LLM\n",
    "\n",
    "    # ÐžÐ±Ñ€Ð°Ñ‚Ð¸Ñ‚Ðµ Ð²Ð½Ð¸Ð¼Ð°Ð½Ð¸Ðµ:\n",
    "    # * Ð’ÑÐµ Ð¿Ð¾Ð»Ñ â€” Ð½ÐµÐ¾Ð±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ (`optional`). Ð­Ñ‚Ð¾ Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÑ‚ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð½Ðµ Ð¸Ð·Ð²Ð»ÐµÐºÐ°Ñ‚ÑŒ Ð½ÐµÐ¾Ð¿Ð¸ÑÐ°Ð½Ð½Ñ‹Ðµ Ð¿Ð¾Ð»Ñ.\n",
    "    # * Ð£ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ð¿Ð¾Ð»Ñ ÐµÑÑ‚ÑŒ Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ðµ (`description`), ÐºÐ¾Ñ‚Ð¾Ñ€Ð¾Ðµ Ð¿ÐµÑ€ÐµÐ´Ð°ÐµÑ‚ÑÑ Ð² Ð¼Ð¾Ð´ÐµÐ»ÑŒ, Ð² Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ð¸ Ð°Ñ€Ð³ÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð² Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸.\n",
    "    # Ð¥Ð¾Ñ€Ð¾ÑˆÐµÐµ Ð¿Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ðµ Ð¿Ð¾Ð¼Ð¾Ð³Ð°ÐµÑ‚ Ð¿Ð¾Ð²Ñ‹ÑÐ¸Ñ‚ÑŒ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ñ.\n",
    "    name: Optional[str] = Field(..., description=\"Ð˜Ð¼Ñ Ñ‡ÐµÐ»Ð¾Ð²ÐµÐºÐ°\")\n",
    "    hair_color: Optional[str] = Field(\n",
    "        ..., description=\"Ð¦Ð²ÐµÑ‚ Ð²Ð¾Ð»Ð¾Ñ Ñ‡ÐµÐ»Ð¾Ð²ÐµÐºÐ°, Ð·Ð°Ð¿Ð¾Ð»Ð½Ð¸ ÐµÑÐ»Ð¸ Ð¸Ð·Ð²ÐµÑÑ‚ÐµÐ½\"\n",
    "    )\n",
    "    height_in_meters: Optional[float] = Field(\n",
    "        ..., description=\"Ð’Ñ‹ÑÐ¾Ñ‚Ð° Ñ‡ÐµÐ»Ð¾Ð²ÐµÐºÐ° Ð² Ð¼ÐµÑ‚Ñ€Ð°Ñ….\"\n",
=======
    "    \"\"\"Information about a person.\"\"\"\n",
    "\n",
    "    # ^ Doc-string for the entity Person.\n",
    "    # This doc-string is sent to the LLM as the description of the schema Person,\n",
    "    # and it can help to improve extraction results.\n",
    "\n",
    "    # Note that:\n",
    "    # 1. Each field is an `optional` -- this allows the model to decline to extract it!\n",
    "    # 2. Each field has a `description` -- this description is used by the LLM.\n",
    "    # Having a good description can help improve extraction results.\n",
    "    name: Optional[str] = Field(default=None, description=\"The name of the person\")\n",
    "    hair_color: Optional[str] = Field(\n",
    "        default=None, description=\"The color of the peron's hair if known\"\n",
    "    )\n",
    "    height_in_meters: Optional[str] = Field(\n",
    "        default=None, description=\"Height measured in meters\"\n",
>>>>>>> langchan/master
    "    )\n",
    "\n",
    "\n",
    "class Data(BaseModel):\n",
<<<<<<< HEAD
    "    \"\"\"Ð˜Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ Ð¾ Ð»ÑŽÐ´ÑÑ….\"\"\"\n",
    "\n",
    "    # Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸, Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ ÐºÐ¾Ñ‚Ð¾Ñ€Ð¾Ð¹ Ð¼Ð¾Ð¶Ð½Ð¾ Ð¸Ð·Ð²Ð»ÐµÐºÐ°Ñ‚ÑŒ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¾ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¸Ñ… Ð»ÑŽÐ´ÑÑ…\n",
=======
    "    \"\"\"Extracted data about people.\"\"\"\n",
    "\n",
    "    # Creates a model so that we can extract multiple entities.\n",
>>>>>>> langchan/master
    "    people: List[Person]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5cda33-fd7b-481e-956a-703f45e40e1d",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    ":::important\n",
    "\n",
    "Ð’ ÑÑ‚Ð¾Ð¼ Ñ€Ð°Ð·Ð´ÐµÐ»Ðµ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½ Ð¾Ð±Ñ‰Ð¸Ð¹ Ð¿Ñ€Ð¸Ð¼ÐµÑ€ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ñ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð¼Ð¾Ð¶ÐµÑ‚ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð½Ðµ ÑÐ°Ð¼Ð¾Ðµ Ð²Ñ‹ÑÐ¾ÐºÐ¾Ðµ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð¾.\n",
    "\n",
    "Ð’ Ñ€Ð°Ð·Ð´ÐµÐ»Ðµ Ð ÑƒÐºÐ¾Ð²Ð¾Ð´ÑÑ‚Ð²Ð° Ð²Ñ‹ Ð½Ð°Ð¹Ð´ÐµÑ‚Ðµ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¾ Ñ‚Ð¾Ð¼ ÐºÐ°Ðº Ð¿Ð¾Ð²Ñ‹ÑÐ¸Ñ‚ÑŒ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ñ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Ð¾Ð±Ñ€Ð°Ð·Ñ†Ð¾Ð²Ñ‹Ñ… Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð¾Ð².\n",
    "\n",
=======
    ":::{.callout-important}\n",
    "Extraction might not be perfect here. Please continue to see how to use **Reference Examples** to improve the quality of extraction, and see the **guidelines** section!\n",
>>>>>>> langchan/master
    ":::"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 22,
=======
   "execution_count": 10,
>>>>>>> langchan/master
   "id": "cf7062cc-1d1d-4a37-9122-509d1b87f0a6",
   "metadata": {},
   "outputs": [
    {
<<<<<<< HEAD
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Giga generation stopped with reason: function_call\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data(people=[Person(name='Ð”Ð¶Ð¾', hair_color='Ñ‡ÐµÑ€Ð½Ñ‹Ð¹', height_in_meters=1.75), Person(name='ÐÐ½Ð½Ð°', hair_color='Ñ‡ÐµÑ€Ð½Ñ‹Ð¹', height_in_meters=1.65)])"
      ]
     },
     "execution_count": 22,
=======
     "data": {
      "text/plain": [
       "Data(people=[Person(name='Jeff', hair_color=None, height_in_meters=None), Person(name='Anna', hair_color=None, height_in_meters=None)])"
      ]
     },
     "execution_count": 10,
>>>>>>> langchan/master
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable = prompt | llm.with_structured_output(schema=Data)\n",
<<<<<<< HEAD
    "text = (\n",
    "    \"ÐœÐ¾Ðµ Ð¸Ð¼Ñ Ð”Ð¶Ð¾, Ð¼Ð¾Ð¸ Ð²Ð¾Ð»Ð¾ÑÑ‹ Ñ‡ÐµÑ€Ð½Ñ‹Ðµ Ð¸ Ñ 1.75 Ð¼ÐµÑ‚Ñ€Ð° Ð²Ñ‹ÑÐ¾Ñ‚Ð¾Ð¹. \"\n",
    "    \"Ð£ ÐÐ½Ð½Ñ‹ Ñ‚Ð°ÐºÐ¸Ðµ Ð¶Ðµ Ð²Ð¾Ð»Ð¾ÑÑ‹ ÐºÐ°Ðº Ñƒ Ð¼ÐµÐ½Ñ Ð¸ Ð¾Ð½Ð° Ð½Ð° 10 ÑÐ°Ð½Ñ‚Ð¸Ð¼ÐµÑ‚Ñ€Ð¾Ð² Ð¼ÐµÐ½ÑŒÑˆÐµ Ð¼ÐµÐ½Ñ.\"\n",
    ")\n",
=======
    "text = \"My name is Jeff, my hair is black and i am 6 feet tall. Anna has the same color hair as me.\"\n",
>>>>>>> langchan/master
    "runnable.invoke({\"text\": text})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba1d770-bf4d-4de4-9e4f-7384872ef0dc",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    ":::note\n",
    "\n",
    "ÐšÐ¾Ð³Ð´Ð° ÑÑ…ÐµÐ¼Ð° Ð¿Ð¾Ð´Ñ€Ð°Ð·ÑƒÐ¼ÐµÐ²Ð°ÐµÑ‚ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¸Ñ… ÑÑƒÑ‰Ð½Ð¾ÑÑ‚ÐµÐ¹, Ð¾Ð½Ð° Ñ‚Ð°ÐºÐ¶Ðµ Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÑ‚ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð½Ðµ Ð¸Ð·Ð²Ð»ÐµÐºÐ°Ñ‚ÑŒ Ð½Ð¸ÐºÐ°ÐºÐ¸Ðµ ÑÑƒÑ‰Ð½Ð¾ÑÑ‚Ð¸ Ð¸ Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°Ñ‚ÑŒ Ð¿ÑƒÑÑ‚Ð¾Ð¹ ÑÐ¿Ð¸ÑÐ¾Ðº, ÐµÑÐ»Ð¸ Ð² Ñ‚ÐµÐºÑÑ‚Ðµ Ð½ÐµÑ‚ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ÑÑ‰Ð¸Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ….\n",
    "\n",
    "ÐšÐ°Ðº Ð¿Ñ€Ð°Ð²Ð¸Ð»Ð¾ ÑÑ‚Ð¾ Ð¿Ð¾Ð»ÐµÐ·Ð½Ð¾, Ñ‚Ð°Ðº ÐºÐ°Ðº Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÑ‚ ÑÐ²Ð½Ð¾ Ð·Ð°Ð´Ð°Ñ‚ÑŒ Ð¾Ð±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð°Ñ‚Ñ€Ð¸Ð±ÑƒÑ‚Ñ‹ ÑÑƒÑ‰Ð½Ð¾ÑÑ‚Ð¸ Ð±ÐµÐ· Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ÑÑ‚Ð¸ Ð²Ñ‹Ð½ÑƒÐ¶Ð´Ð°Ñ‚ÑŒ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð¾Ð±Ð½Ð°Ñ€ÑƒÐ¶Ð¸Ð²Ð°Ñ‚ÑŒ Ñ‚Ð°ÐºÑƒÑŽ ÑÑƒÑ‰Ð½Ð¾ÑÑ‚ÑŒ.\n",
    "\n",
    ":::"
=======
    ":::{.callout-tip}\n",
    "When the schema accommodates the extraction of **multiple entities**, it also allows the model to extract **no entities** if no relevant information\n",
    "is in the text by providing an empty list. \n",
    "\n",
    "This is usually a **good** thing! It allows specifying **required** attributes on an entity without necessarily forcing the model to detect this entity.\n",
    ":::\n",
    "\n",
    "We can see the LangSmith trace here: https://smith.langchain.com/public/7173764d-5e76-45fe-8496-84460bd9cdef/r"
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07a7455-7de6-4a6f-9772-0477ef65e3dc",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## Ð¡Ð¼Ð¾Ñ‚Ñ€Ð¸Ñ‚Ðµ Ñ‚Ð°ÐºÐ¶Ðµ\n",
    "\n",
    "* Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ [Ð¾Ð±Ñ€Ð°Ð·Ñ†Ñ‹ ÐºÐ¾Ð´Ð°](/docs/use_cases/extraction/how_to/examples) Ð´Ð»Ñ Ð±Ð¾Ð»ÐµÐµ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾Ð³Ð¾ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ñ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸.\n",
    "* Ð£Ð·Ð½Ð°Ð¹Ñ‚Ðµ [Ñ‡Ñ‚Ð¾ Ð´ÐµÐ»Ð°Ñ‚ÑŒ](/docs/use_cases/extraction/how_to/handle_long_text), ÐºÐ¾Ð³Ð´Ð° Ñ€Ð°Ð·Ð¼ÐµÑ€ Ñ‚ÐµÐºÑÑ‚Ð° Ð¿Ñ€ÐµÐ²Ñ‹ÑˆÐ°ÐµÑ‚ Ð¾Ð±ÑŠÐµÐ¼ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð° Ð¼Ð¾Ð´ÐµÐ»Ð¸.\n",
    "* Ð˜Ð·ÑƒÑ‡Ð¸Ñ‚Ðµ ÐºÐ°Ðº Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ð·Ð°Ð³Ñ€ÑƒÐ·Ñ‡Ð¸ÐºÐ¸ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð² Ð¸ Ð¿Ð°Ñ€ÑÐµÑ€Ñ‹ GigaChain Ð´Ð»Ñ [Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ñ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸Ð· Ñ„Ð°Ð¹Ð»Ð¾Ð²](/docs/use_cases/extraction/how_to/handle_files) Ð²Ñ€Ð¾Ð´Ðµ PDF.\n",
    "* ÐžÐ·Ð½Ð°ÐºÐ¾Ð¼ÑŒÑ‚ÐµÑÑŒ Ñ [Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð¾Ð¼ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ñ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ð¹ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸](/docs/use_cases/extraction/how_to/parse) Ð¾Ñ‚ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð½Ðµ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÑŽÑ‚ Ñ€Ð°Ð±Ð¾Ñ‚Ñƒ Ñ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ð°Ð¼Ð¸ Ð¸Ð»Ð¸ Ñ„ÑƒÐ½ÐºÑ†Ð¸ÑÐ¼Ð¸. "
=======
    "## Next steps\n",
    "\n",
    "Now that you understand the basics of extraction with LangChain, you're ready to proceed to the rest of the how-to guides:\n",
    "\n",
    "- [Add Examples](/docs/how_to/extraction_examples): Learn how to use **reference examples** to improve performance.\n",
    "- [Handle Long Text](/docs/how_to/extraction_long_text): What should you do if the text does not fit into the context window of the LLM?\n",
    "- [Use a Parsing Approach](/docs/how_to/extraction_parse): Use a prompt based approach to extract with models that do not support **tool/function calling**."
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deb47ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
